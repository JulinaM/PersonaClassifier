{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re,os, glob, traceback, nltk\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging, sys\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "logging.basicConfig(filename=f'log/jp_log_{timestamp}.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID                       STATUS  sEXT  sNEU  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4  likes the sound of thunder.  2.65   3.0   \n",
       "\n",
       "   sAGR  sCON  sOPN  cEXT  cNEU  cAGR  cCON  cOPN               DATE  \\\n",
       "0  3.15  3.25   4.4     0     1     0     0     1  06/19/09 03:21 PM   \n",
       "\n",
       "   NETWORKSIZE  BETWEENNESS  NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  \\\n",
       "0        180.0      14861.6         93.29     0.03    15661.0        0.49   \n",
       "\n",
       "   TRANSITIVITY  \n",
       "0           0.1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_personality_file = 'data/mypersonality.csv'\n",
    "main_df = pd.read_csv(my_personality_file, encoding='Windows-1252')\n",
    "# main_df.drop(columns=['#AUTHID', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'DATE', 'NETWORKSIZE', 'BETWEENNESS', 'NBETWEENNESS', 'DENSITY', 'BROKERAGE', 'NBROKERAGE','TRANSITIVITY'], inplace=True)\n",
    "main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']] = main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']].replace({'y': 1, 'n': 0})\n",
    "logging.info(f'File={my_personality_file} shape={main_df.shape}')\n",
    "main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_file = 'data/LIWC_mypersonality_oct_2.csv'\n",
    "liwc_df = pd.read_csv(liwc_file)\n",
    "liwc_df = liwc_df.drop(['Unnamed: 0', 'ColumnID', 'Text'], axis=1)\n",
    "# liwc_df.fillna(value=np.nan, inplace=True)\n",
    "# numerical_columns = liwc_df.select_dtypes(include=[np.number]).columns\n",
    "# liwc_df[numerical_columns] = liwc_df[numerical_columns].fillna(liwc_df[numerical_columns].mean())\n",
    "# liwc_df[numerical_columns] = liwc_df[numerical_columns].fillna(0)\n",
    "# liwc_df.fillna(value=0, inplace=True)\n",
    "logging.info(f'File={liwc_file} shape={liwc_df.shape}')\n",
    "liwc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>Segment</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  cEXT  cNEU  cAGR  cCON  cOPN  Segment  WC  \\\n",
       "0  likes the sound of thunder.     0     1     0     0     1        1   5   \n",
       "\n",
       "   Analytic  Clout  ...  nonflu  filler  AllPunc  Period  Comma  QMark  \\\n",
       "0      99.0    NaN  ...     0.0     0.0     20.0    20.0    0.0    0.0   \n",
       "\n",
       "   Exclam  Apostro  OtherP  Emoji  \n",
       "0     0.0      0.0     0.0    0.0  \n",
       "\n",
       "[1 rows x 125 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.merge(main_df, liwc_df, on=\"#AUTHID\", how=\"left\")\n",
    "df = pd.concat([main_df, liwc_df], axis=1)\n",
    "df = df.drop(['#AUTHID'], axis=1)\n",
    "str(list(df.columns))\n",
    "# df.dropna(inplace=True)\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(main_file, liwc_file, is_mypersonality=True):\n",
    "    if is_mypersonality:\n",
    "        main_df = pd.read_csv(main_file, encoding='Windows-1252')\n",
    "        main_df.drop(columns=['#AUTHID', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'DATE', 'NETWORKSIZE', 'BETWEENNESS', 'NBETWEENNESS', 'DENSITY', 'BROKERAGE', 'NBROKERAGE','TRANSITIVITY'], inplace=True)\n",
    "        main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']] = main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']].replace({'y': 1, 'n': 0})\n",
    "    else:\n",
    "        main_df = pd.read_csv(main_file)\n",
    "        cols = ['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']\n",
    "        for col in cols:\n",
    "            mean_value = main_df[col].mean()\n",
    "            main_df[f'{col}'] = main_df[col] > mean_value\n",
    "        main_df[cols] = main_df[cols].replace({True: 1, False: 0})  \n",
    "    logging.info(f'File={main_file} shape={main_df.shape}')\n",
    "\n",
    "    liwc_df = pd.read_csv(liwc_file)\n",
    "    liwc_df = liwc_df.drop(['Unnamed: 0', 'ColumnID', 'STATUS'] if is_mypersonality else ['Unnamed: 0', 'STATUS'], axis=1)\n",
    "    logging.info(f'File={liwc_file} shape={liwc_df.shape}')\n",
    "\n",
    "    df = pd.concat([main_df, liwc_df], axis=1)\n",
    "    df = df.drop(['#AUTHID'], axis=1)\n",
    "    logging.info(f'Final shape after concat={df.shape}')\n",
    "    return df\n",
    "\n",
    "# fb_df = read_data('data/mypersonality.csv', 'data/LIWC_mypersonality_oct_2.csv')\n",
    "df = read_data('data/pandora_to_big5.csv', 'data/LIWC_pandora_to_big5_oct_24.csv', False)\n",
    "# df = pd.concat([fb_df, rd_df], ignore_index=True)\n",
    "\n",
    "logging.info(f'Combined shape ={df.shape}')\n",
    "logging.info(40*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_vad = pd.read_csv('data/NRC-VAD-Lexicon/NRC-VAD-Lexicon.csv', sep=\"\\t\")  \n",
    "nrc_vad_dict = nrc_vad.set_index('Word').to_dict(orient='index')\n",
    "def get_vad_scores(text):\n",
    "    words = text.split()\n",
    "    valence_scores, arousal_scores, dominance_scores = [], [], []\n",
    "    for word in words:\n",
    "        word = word.lower()  # Lowercase to match the lexicon\n",
    "        if word in nrc_vad_dict:\n",
    "            vad_values = nrc_vad_dict[word]\n",
    "            valence_scores.append(vad_values['Valence'])\n",
    "            arousal_scores.append(vad_values['Arousal'])\n",
    "            dominance_scores.append(vad_values['Dominance'])\n",
    "    if not valence_scores:\n",
    "        return {'Valence': 0, 'Arousal': 0, 'Dominance': 0}\n",
    "\n",
    "    valence_avg = sum(valence_scores) / len(valence_scores)\n",
    "    arousal_avg = sum(arousal_scores) / len(arousal_scores)\n",
    "    dominance_avg = sum(dominance_scores) / len(dominance_scores)\n",
    "    return {'Valence': valence_avg, 'Arousal': arousal_avg, 'Dominance': dominance_avg}\n",
    "\n",
    "df['VAD_Scores'] = df['STATUS'].apply( lambda x: get_vad_scores(x))\n",
    "df[['Valence', 'Arousal', 'Dominance']] = pd.DataFrame(df['VAD_Scores'].tolist(), index=df.index)\n",
    "df.drop(columns=['VAD_Scores'], inplace=True)\n",
    "logging.info(f'NRC-VAD shape={df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jmaharja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nrc_lexicon = pd.read_csv('data/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', names=[\"word\", \"emotion\", \"association\"],sep=\"\\t\", header=None)\n",
    "# Filter out words that have no association with emotions (association == 0)\n",
    "nrc_lexicon = nrc_lexicon[nrc_lexicon['association'] == 1]\n",
    "# nrc_lexicon.drop(columns=['association'], inplace=True)\n",
    "nrc_pivot = nrc_lexicon.pivot(index=\"word\", columns=\"emotion\", values=\"association\").fillna(0).astype(int)\n",
    "# nrc_pivot.head(2)\n",
    "nltk.download('punkt')\n",
    "def get_emotion_counts(text, lexicon):\n",
    "    # print(text)\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    emotion_count = defaultdict(int)\n",
    "    for word in words:\n",
    "        if word in lexicon.index:\n",
    "            for emotion in lexicon.columns:\n",
    "                emotion_count[emotion] += lexicon.loc[word, emotion]\n",
    "    return emotion_count\n",
    "emotion_counts_list = df['STATUS'].apply(lambda x: get_emotion_counts(x, nrc_pivot))\n",
    "emotion_counts_df = pd.DataFrame(emotion_counts_list.tolist())\n",
    "emotion_counts_df.fillna(0, inplace=True)\n",
    "emotion_counts_df = emotion_counts_df.astype(int)\n",
    "df = pd.concat([df, emotion_counts_df], axis=1)\n",
    "logging.info(f'NRC-Emotion shape={df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def find_sentiment(text):\n",
    "    # print(text)\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    sc = vs['compound']\n",
    "    # emo = 'pos' if sc >= 0.05 else 'neu' if -0.05 < sc < 0.05 else 'neg'\n",
    "    return sc\n",
    "df[['sent_score']] = df['STATUS'].apply(lambda x: pd.Series(find_sentiment(x)))\n",
    "logging.info(f'VADER shape={df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TFIDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # Step 1: Initialize the TfidfVectorizer\n",
    "# # You can specify parameters like max_features, ngram_range, etc., based on your needs\n",
    "# tfidf = TfidfVectorizer(max_features=100, stop_words='english')  # Adjust max_features as necessary\n",
    "\n",
    "# # Step 2: Fit and transform the 'STATUS' column\n",
    "# # This step converts the text in 'STATUS' to TF-IDF features\n",
    "# tfidf_matrix = tfidf.fit_transform(df['STATUS'].astype(str))  # Ensure 'STATUS' column is in string format\n",
    "\n",
    "# # Step 3: Convert the TF-IDF matrix into a DataFrame\n",
    "# # The resulting matrix is sparse, so we'll convert it to a DataFrame with feature names\n",
    "# tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# # Step 4: Optionally, merge the TF-IDF features back with your original DataFrame\n",
    "# # This will add the new TF-IDF feature columns to your existing DataFrame\n",
    "# df = pd.concat([df, tfidf_df], axis=1)\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_459199/2673855340.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['roberta_embeddings'] = list(roberta_embeddings)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_embeddings(df, model_name, batch_size=8):\n",
    "    logging.info(f'Embeding : {model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_texts = df['STATUS'][i:i + batch_size].tolist()\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings_list.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings_list)\n",
    "    \n",
    "# bert_embeddings = get_embeddings(df, 'bert-base-uncased', batch_size=2)\n",
    "roberta_embeddings = get_embeddings(df, 'roberta-base', batch_size=2)\n",
    "# berttweet_embeddings = get_embeddings(df, 'vinai/bertweet-base', batch_size=2)\n",
    "# xlnet_embeddings = get_embeddings(df, 'xlnet-base-cased', batch_size=2)\n",
    "# df['bert_embeddings'] = list(bert_embeddings)\n",
    "df['roberta_embeddings'] = list(roberta_embeddings)\n",
    "# df['berttweet_embeddings'] = list(berttweet_embeddings)\n",
    "# df['xlnet_embeddings'] = list(xlnet_embeddings)\n",
    "# df[['STATUS', 'bert_embeddings', 'roberta_embeddings', 'berttweet_embeddings', 'xlnet_embeddings']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = df.columns\n",
    "remove_cols = ['STATUS', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "scaler = StandardScaler() \n",
    "stat_features = df[stat_cols]\n",
    "stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "# stat_features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19006328, -0.12640104,  1.24846165, ..., -0.10689668,\n",
       "        -0.03087542, -0.05612235],\n",
       "       [-0.19006328, -0.12640104, -0.37582433, ..., -0.0694667 ,\n",
       "        -0.03203625, -0.02442368],\n",
       "       [-0.19006328, -0.12640104, -0.75957689, ..., -0.09310248,\n",
       "        -0.05732121, -0.07783641],\n",
       "       ...,\n",
       "       [-0.19006328, -0.02101195, -0.88349698, ..., -0.09929476,\n",
       "        -0.03069092, -0.02944294],\n",
       "       [-0.19006328, -0.12640104, -1.08603305, ..., -0.04976954,\n",
       "        -0.02862299, -0.02082449],\n",
       "       [-0.19006328, -0.12640104, -0.83719351, ..., -0.09859692,\n",
       "        -0.0528679 , -0.08392557]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# roberta_embeddings\n",
    "# np.array(roberta_embeddings.tolist())\n",
    "X = np.concatenate([stat_features_scaled, roberta_embeddings], axis=1)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'data/my_personality_all_embs.csv'\n",
    "# df.to_csv(filename)\n",
    "# logging.info(f'saving to {filename}')\n",
    "# # df = pd.read_csv(\"data/my_personality_all_embs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()\n",
    "# dff.fillna(value=np.nan, inplace=True)\n",
    "# numerical_columns = dff.select_dtypes(include=[np.number]).columns\n",
    "# dff[numerical_columns] = dff[numerical_columns].fillna(dff[numerical_columns].mean())\n",
    "dff.fillna(value=0, inplace=True)\n",
    "logging.info(f'Total shape={dff.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "    def forward(self, x):\n",
    "        query = x[:, -1:, :]  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        scores = torch.bmm(query, x.transpose(1, 2))  # Shape: (batch_size, 1, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.bmm(attention_weights, x)  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, do_attention=True, dropout_rate=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.do_attention = do_attention\n",
    "        self.attention = DotProductAttention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim) \n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "    def forward(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            x = x.unsqueeze(1)  \n",
    "        if self.do_attention:\n",
    "            context_vector, attention_weights = self.attention(x)\n",
    "            context_vector = self.layer_norm1(context_vector)\n",
    "        else:\n",
    "            context_vector = x\n",
    "        lstm_output, _ = self.lstm(context_vector)     \n",
    "        lstm_output = self.layer_norm2(lstm_output)\n",
    "        last_hidden_state = lstm_output[:, -1, :]  # Shape: (batch_size, hidden_dim * 2)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        output = self.fc(last_hidden_state)  \n",
    "        return output\n",
    "\n",
    "# # Test model Initialize model\n",
    "# model = BiLSTMClassifier(input_dim=768, hidden_dim=128, output_dim=5, num_layers=2)\n",
    "# input_data = torch.randn(2, 5, 768)  # Example input (batch_size=32, seq_len=50, input_dim=768)\n",
    "# output= model(input_data)\n",
    "# print(output.shape)  # Expected output: (batch_size, output_dim)\n",
    "\n",
    "def train_val_dl_models(model, train_loader, val_loader, max_grad_norm=1.0, epochs=16, lr=0.001):\n",
    "    logging.info(f'{model.__class__.__name__}')\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for t, labels in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            outputs= model(t)\n",
    "            loss = criterion(outputs, labels)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        model.eval()  \n",
    "        val_preds, val_labels, val_loss = [], [], 0\n",
    "        with torch.no_grad():  \n",
    "            for inputs, labels in val_loader:\n",
    "                outputs= model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()  \n",
    "                val_preds.append(torch.sigmoid(outputs))  \n",
    "                val_labels.append(labels) \n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "        val_preds = (val_preds > 0.5).float() \n",
    "        val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "        if epoch % 4 == 0:\n",
    "            logging.info(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {total_loss / len(train_loader):.4f}, 'f'Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    return val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_training_class:\n",
    "    def __init__(self, dff):\n",
    "        self.dff = dff\n",
    "        # self.output_df = pd.DataFrame()\n",
    "     \n",
    "    def prepare_dataset(self, embedding_type, target_col):\n",
    "        all_cols = self.dff.columns\n",
    "        remove_cols = ['STATUS', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "        emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "        stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "        if embedding_type:\n",
    "            contextual_embeddings = np.array(self.dff[embedding_type].tolist())\n",
    "        scaler = StandardScaler() \n",
    "        stat_features = self.dff[stat_cols]\n",
    "        stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "        X = np.concatenate([stat_features_scaled, contextual_embeddings] if embedding_type else [stat_features_scaled], axis=1)\n",
    "        y = np.array(self.dff[[target_col]]) \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        print(self.X_train.shape, self.y_train.shape)\n",
    "        self.input_dim = self.X_train.shape[1]\n",
    "        # X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "        # logging.info(f'Train  size: {X_train.shape}, Val size: {X_val.shape}, Test  size: {X_test.shape}')\n",
    "        X_train_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(self.y_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(self.X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(self.y_val, dtype=torch.float32)\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    def init_models(self):\n",
    "        self.svm_model = SVC(kernel='linear')\n",
    "        self.lr_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        self.rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "        self.bilstm_model = BiLSTMClassifier(input_dim=self.input_dim, hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.5)\n",
    "        self.mlp_model = MLP(input_size=self.input_dim, hidden_size=128, output_size=1, dropout_rate=0.3)\n",
    "    \n",
    "    def fit_validate_and_generate_acc_scr(self):\n",
    "        self.svm_model.fit(self.X_train, self.y_train)\n",
    "        self.lr_model.fit(self.X_train, self.y_train)\n",
    "        self.rf_model.fit(self.X_train, self.y_train)\n",
    "        self.xgb_model.fit(self.X_train, self.y_train)\n",
    "        mlp_acc = train_val_dl_models(self.mlp_model, self.train_loader, self.val_loader)\n",
    "        bilstm_acc = train_val_dl_models(self.bilstm_model, self.train_loader, self.val_loader)\n",
    "\n",
    "        svm_y_pred = self.svm_model.predict(self.X_val)\n",
    "        lr_y_pred = self.lr_model.predict(self.X_val)\n",
    "        rf_y_pred = self.rf_model.predict(self.X_val)\n",
    "        xgb_y_pred = self.xgb_model.predict(self.X_val)\n",
    "        svm_accuracy = accuracy_score(self.y_val, svm_y_pred)\n",
    "        lr_accuracy = accuracy_score(self.y_val, lr_y_pred)\n",
    "        rf_accuracy = accuracy_score(self.y_val, rf_y_pred)\n",
    "        xgb_accuracy = accuracy_score(self.y_val, xgb_y_pred)\n",
    "\n",
    "        logging.info(f'SVM Val Acc: {svm_accuracy:.2f}')\n",
    "        logging.info(f'LR Val Acc: {lr_accuracy:.2f}')\n",
    "        logging.info(f'RF Val Acc: {rf_accuracy:.2f}')\n",
    "        logging.info(f'SGBoost Val Acc: {xgb_accuracy:.2f}')\n",
    "        logging.info(f'MLP Val Acc: {mlp_acc:.2f}')\n",
    "        logging.info(f'BiLSTM Val Acc: {bilstm_acc:.2f}')\n",
    "\n",
    "    # def test_model():\n",
    "    #     model.eval()\n",
    "    #     X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    #     with torch.no_grad():  \n",
    "    #         outputs = model(X_test_tensor)\n",
    "    #     outputs = torch.sigmoid(outputs)\n",
    "    #     preds = (outputs > 0.5).float()\n",
    "    #     preds_np = preds.numpy()\n",
    "    #     y_test_np = y_test  # If y_test is already in numpy format, otherwise y_test.numpy()\n",
    "    #     accuracy = accuracy_score(y_test_np, preds_np)\n",
    "    #     precision = precision_score(y_test_np, preds_np)\n",
    "    #     recall = recall_score(y_test_np, preds_np)\n",
    "    #     f1 = f1_score(y_test_np, preds_np)\n",
    "            # Print metrics\n",
    "            # print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    def train_all_models(self,embedding_type):\n",
    "        logging.info(f'Training started ::')\n",
    "        for target_cols in ['cOPN', 'cCON', 'cEXT', 'cAGR', 'cNEU']:\n",
    "            logging.info(f'Trait: {target_cols}')\n",
    "            self.prepare_dataset(embedding_type, target_cols)\n",
    "            self.init_models()\n",
    "            self.fit_validate_and_generate_acc_scr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (90, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m my_train \u001b[38;5;241m=\u001b[39m My_training_class(dff)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmy_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroberta_embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 85\u001b[0m, in \u001b[0;36mMy_training_class.train_all_models\u001b[0;34m(self, embedding_type)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_dataset(embedding_type, target_cols)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_models()\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_validate_and_generate_acc_scr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 40\u001b[0m, in \u001b[0;36mMy_training_class.fit_validate_and_generate_acc_scr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_validate_and_generate_acc_scr\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrf_model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/sklearn/svm/_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/sklearn/utils/validation.py:1163\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[1;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1163\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/sklearn/utils/validation.py:1184\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1183\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1184\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[1;32m   1186\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/sklearn/utils/validation.py:1245\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1235\u001b[0m             (\n\u001b[1;32m   1236\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1241\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1242\u001b[0m         )\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m-> 1245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1247\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (90, 2) instead."
     ]
    }
   ],
   "source": [
    "my_train = My_training_class(dff)\n",
    "my_train.train_all_models('roberta_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiLSTMClsAttn(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, do_attention=True, dropout_rate=0.5):\n",
    "#         super(BiLSTMClsAttn, self).__init__()\n",
    "#         self.do_attention = do_attention\n",
    "#         self.attention = DotProductAttention(hidden_dim)\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout_rate)\n",
    "#         self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "#         self.layer_norm1 = nn.LayerNorm(input_dim) \n",
    "#         self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)  \n",
    "#         self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if len(x.size()) == 2:\n",
    "#             x = x.unsqueeze(1)  \n",
    "#         lstm_output, _ = self.lstm(x)     \n",
    "#         lstm_output = self.layer_norm2(lstm_output)\n",
    "#         last_hidden_state = lstm_output[:, -1, :]  # Shape: (batch_size, hidden_dim * 2)\n",
    "#         last_hidden_state = self.dropout(last_hidden_state)\n",
    "#         context_vector, attention_weights = self.attention(last_hidden_state)\n",
    "#         context_vector = self.layer_norm1(context_vector)\n",
    "#         output = self.fc(context_vector)  \n",
    "#         return output\n",
    "\n",
    "# model = BiLSTMClsAttn(input_dim=768, hidden_dim=128, output_dim=1, num_layers=2)\n",
    "# input_data = torch.randn(2, 5, 768)  # Example input (batch_size=32, seq_len=50, input_dim=768)\n",
    "# output= model(input_data)\n",
    "# print(output.shape)  # Expected output: (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BiLSTMClassifier(input_dim=X_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.5)\n",
    "# # model = MLP(input_size=X_train.shape[1], hidden_size=128, output_size=1, dropout_rate=0.3)\n",
    "# criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "# num_epochs = 16\n",
    "# max_grad_norm=1.0\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     for t, labels in train_loader:\n",
    "#         optimizer.zero_grad()  \n",
    "#         outputs= model(t)\n",
    "#         # print(outputs.shape, labels.shape, t.shape)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     # Validation phase\n",
    "#     model.eval()  \n",
    "#     val_preds = []\n",
    "#     val_labels = []\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():  \n",
    "#         for inputs, labels in val_loader:\n",
    "#             outputs= model(inputs)\n",
    "#             val_loss += criterion(outputs, labels).item()  \n",
    "#             val_preds.append(torch.sigmoid(outputs))  \n",
    "#             val_labels.append(labels) \n",
    "#     val_preds = torch.cat(val_preds)\n",
    "#     val_labels = torch.cat(val_labels)\n",
    "#     val_preds = (val_preds > 0.5).float() \n",
    "#     val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "#           f'Train Loss: {total_loss / len(train_loader):.4f}, '\n",
    "#           f'Validation Loss: {val_loss / len(val_loader):.4f}, '\n",
    "#           f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7248, Precision: 0.6162, Recall: 0.5351, F1 Score: 0.5728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():  \n",
    "    outputs = model(X_test_tensor)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "preds = (outputs > 0.5).float()\n",
    "preds_np = preds.numpy()\n",
    "y_test_np = y_test  # If y_test is already in numpy format, otherwise y_test.numpy()\n",
    "accuracy = accuracy_score(y_test_np, preds_np)\n",
    "precision = precision_score(y_test_np, preds_np)\n",
    "recall = recall_score(y_test_np, preds_np)\n",
    "f1 = f1_score(y_test_np, preds_np)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['new', 'Authentic', 'right', 'joy', 'det', 'excited', 'illness', 'adj', 'car', 'auditory', 'feel', 'hours', 'old', 'watching', 'Emoji', 'Period', 'they', 'want', 'auxverb', 'socbehav', 'power', 'emo_neg', 'sadness', 'socrefs', 'tomorrow', 'negative', 'family', 'tech', 'Conversation', 'don', 'Social', 'substances', 'filler', 'day', 'emotion', 'night', 'going', 'trust', 'Drives', 'days', 'oh', 'Cognition', 'think', 'need', 'mental', 'ipron', 'memory', 'finally', 'trying', 'Tone', 'gonna', 'WC', 'NETWORKSIZE', 'Physical', 'surprise', 'Arousal', 'god', 'propname', 'sleep', 'focuspresent', 'ppron', 'prep', 'death', 'week', 'yay', 'year', 'Linguistic', 'Apostro', 'did', 'comm', 'politic', 'doing', 'getting', 'friends', 'moral', 'needs', 'come', 'loves', 'fun', 'sexual', 'thank', 'BETWEENNESS', 'better', 'prosocial', 'working', 'sent_score', 'BigWords', 'female', 'bed', 'weekend', 'life', 'motion', 'happy', 'article', 'certitude', 'ready', 'sick', 'best', 'food', 'christmas', 'love', 'nonflu', 'today', 'differ', 've', 'disgust', 'insight', 'lack', 'home', 'birthday', 'conj', 'fear', 'BROKERAGE', 'shehe', 'feeling', 'hope', 'just', 'thinks', 'like', 'cause', 'tone_neg', 'bad', 'looking', 'you', 'male', 'does', 'visual', 'let', 'fatigue', 'NBETWEENNESS', 'swear', 'awesome', 'achieve', 'snow', 'Affect', 'verb', 'man', 'Lifestyle', 'having', 'wellness', 'Exclam', 'affiliation', 'Comma', 'class', 'ethnicity', 'anticipation', 'space', 'lol', 'cogproc', 'Perception', 'risk', 'tone_pos', 'Analytic', 'time', 'Dominance', 'positive', 'know', 'll', 'make', 'tonight', 'emo_anx', 'allure', 'AllPunc', 'watch', 'fulfill', 'emo_sad', 'school', 'quantity', 'friend', 'number', 'QMark', 'morning', 'say', 'wishes', 'conflict', 'people', 'allnone', 'im', 'work', 'reward', 'thanks', 'acquire', 'got', 'things', 'way', 'world', 'money', 'WPS', 'netspeak', 'really', 'OtherP', 'friday', 'long', 'wants', 'thing', 'anger', 'great', 'pronoun', 'emo_anger', 'Valence', 'DENSITY', 'negate', 'making', 'tired', 'relig', 'focusfuture', 'i', 'leisure', 'little', 'Dic', 'house', 'good', 'adverb', 'function', 'tentat', 'start', 'TRANSITIVITY', 'attention', 'Segment', 'we', 'health', 'assent', 'curiosity', 'doesn', 'Culture', 'facebook', 'Clout', 'discrep', 'polite', 'focuspast', 'maybe', 'wait', 'emo_pos', 'NBROKERAGE', 'la']\",\n",
       " 235)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(dff, test_size=0.1, random_state=42)\n",
    "print(f'Train set size: {train_df.shape}')\n",
    "print(f'Test set size: {test_df.shape}')all_cols = dff.columns\n",
    "\n",
    "# label_cols = ['cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "label_cols = [\"cCON\"]\n",
    "remove_cols = ['#AUTHID', 'STATUS', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN', 'DATE']\n",
    "emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "\n",
    "stat_features = train_df[stat_cols]\n",
    "bert_embeddings = np.array(train_df[\"bert_embeddings\"].tolist())\n",
    "roberta_embeddings = np.array(train_df[\"roberta_embeddings\"].tolist())\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "X1 = np.concatenate([stat_features_scaled, bert_embeddings], axis=1)\n",
    "X2 = np.concatenate([stat_features_scaled, roberta_embeddings], axis=1)\n",
    "y = np.array(train_df[label_cols]) \n",
    "\n",
    "# Split data into train+val and test sets (80% train+val, 20% test)\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1, y, test_size=0.1, random_state=42)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset1 = TensorDataset(torch.tensor(X1_train, dtype=torch.float32), torch.tensor(y1_train, dtype=torch.float32))\n",
    "val_dataset1 = TensorDataset(torch.tensor(X1_val, dtype=torch.float32), torch.tensor(y1_val, dtype=torch.float32))\n",
    "train_dataset2 = TensorDataset(torch.tensor(X2_train, dtype=torch.float32), torch.tensor(y2_train, dtype=torch.float32))\n",
    "val_dataset2 = TensorDataset(torch.tensor(X2_val, dtype=torch.float32), torch.tensor(y2_val, dtype=torch.float32))\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=32, shuffle=True)\n",
    "val_loader1 = DataLoader(val_dataset1, batch_size=32, shuffle=False)\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=32, shuffle=True)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Train Loss Model1: 0.6946, Validation Loss Model1: 0.7014, Validation Loss Model2: 0.7130, Validation Accuracy: 0.5566\n",
      "Epoch [2/12], Train Loss Model1: 0.6414, Validation Loss Model1: 0.7564, Validation Loss Model2: 0.7003, Validation Accuracy: 0.5924\n",
      "Epoch [3/12], Train Loss Model1: 0.5967, Validation Loss Model1: 0.8415, Validation Loss Model2: 0.6924, Validation Accuracy: 0.5969\n",
      "Epoch [4/12], Train Loss Model1: 0.5288, Validation Loss Model1: 0.9173, Validation Loss Model2: 0.6923, Validation Accuracy: 0.6036\n",
      "Epoch [5/12], Train Loss Model1: 0.4329, Validation Loss Model1: 1.1190, Validation Loss Model2: 0.6883, Validation Accuracy: 0.6047\n",
      "Epoch [6/12], Train Loss Model1: 0.3463, Validation Loss Model1: 1.3680, Validation Loss Model2: 0.6925, Validation Accuracy: 0.6036\n",
      "Epoch [7/12], Train Loss Model1: 0.2609, Validation Loss Model1: 1.5066, Validation Loss Model2: 0.6901, Validation Accuracy: 0.6260\n",
      "Epoch [8/12], Train Loss Model1: 0.2033, Validation Loss Model1: 1.9807, Validation Loss Model2: 0.6896, Validation Accuracy: 0.6237\n",
      "Epoch [9/12], Train Loss Model1: 0.1579, Validation Loss Model1: 2.2688, Validation Loss Model2: 0.6886, Validation Accuracy: 0.6159\n",
      "Epoch [10/12], Train Loss Model1: 0.1339, Validation Loss Model1: 2.3395, Validation Loss Model2: 0.6916, Validation Accuracy: 0.6170\n",
      "Epoch [11/12], Train Loss Model1: 0.1023, Validation Loss Model1: 2.4648, Validation Loss Model2: 0.7034, Validation Accuracy: 0.6327\n",
      "Epoch [12/12], Train Loss Model1: 0.0922, Validation Loss Model1: 2.7707, Validation Loss Model2: 0.6888, Validation Accuracy: 0.6215\n"
     ]
    }
   ],
   "source": [
    "#Model Aveaging\n",
    "model1 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "model2 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "\n",
    "# model1 = MLP(input_size=X1_train.shape[1], hidden_size=128, output_size=1)\n",
    "# model2 = MLP(input_size=X2_train.shape[1], hidden_size=128, output_size=1)\n",
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "num_epochs = 12\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for (t1, labels1), (t2, labels2) in zip(train_loader1, train_loader2):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        outputs1 = model1(t1)\n",
    "        outputs2 = model2(t2)\n",
    "        \n",
    "        avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "        loss = criterion(avg_outputs, labels1)  \n",
    "        torch.nn.utils.clip_grad_norm_(model1.parameters(), max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        total_loss += loss.item()\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss1 = 0\n",
    "    val_loss2 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (inputs1, labels1), (inputs2, labels2) in zip(val_loader1, val_loader2):\n",
    "            outputs1 = model1(inputs1)\n",
    "            outputs2 = model2(inputs2)\n",
    "            avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "            val_loss1 += criterion(outputs1, labels1).item()\n",
    "            val_loss2 += criterion(outputs2, labels2).item()\n",
    "            val_preds.append(torch.sigmoid(avg_outputs))\n",
    "            val_labels.append(labels1)\n",
    "    \n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_preds = (val_preds > 0.5).float()\n",
    "    val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss Model1: {total_loss / len(train_loader1):.4f}, '\n",
    "          f'Validation Loss Model1: {val_loss1 / len(val_loader1):.4f}, '\n",
    "          f'Validation Loss Model2: {val_loss2 / len(val_loader2):.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
