{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re,os, glob, traceback, nltk\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging, sys\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "logging.basicConfig(filename=f'log/jp_log_{timestamp}.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('data/pandora_to_big5.csv')\n",
    "main_df.drop(columns=['#AUTHID'], inplace=True)\n",
    "cols = ['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']\n",
    "for col in cols:\n",
    "    mean_value = main_df[col].mean()\n",
    "    main_df[f'{col}'] = main_df[col] > mean_value\n",
    "    main_df[cols] = main_df[cols].replace({True: 1, False: 0})  \n",
    "\n",
    "liwc_df = pd.read_csv('data/LIWC_pandora_to_big5_oct_24.csv')\n",
    "liwc_df = liwc_df.drop(['Unnamed: 0', '#AUTHID', 'STATUS', 'cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON', ], axis=1)\n",
    "\n",
    "# main_df = pd.read_csv(main_file, encoding='Windows-1252')\n",
    "# main_df.drop(columns=['#AUTHID', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'DATE', 'NETWORKSIZE', 'BETWEENNESS', 'NBETWEENNESS', 'DENSITY', 'BROKERAGE', 'NBROKERAGE','TRANSITIVITY'], inplace=True)\n",
    "# main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']] = main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']].replace({'y': 1, 'n': 0})\n",
    "# liwc_df = pd.read_csv(liwc_file)\n",
    "# liwc_df = liwc_df.drop(['Unnamed: 0', '#AUTHID', 'ColumnID', 'STATUS'], axis=1)\n",
    "\n",
    "df = pd.concat([main_df, liwc_df], axis=1)\n",
    "logging.info(f'Merged main and liwc files, Shape:{df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_vad = pd.read_csv('data/NRC-VAD-Lexicon/NRC-VAD-Lexicon.csv', sep=\"\\t\")  \n",
    "nrc_vad_dict = nrc_vad.set_index('Word').to_dict(orient='index')\n",
    "def get_vad_scores(text):\n",
    "    words = text.split()\n",
    "    valence_scores, arousal_scores, dominance_scores = [], [], []\n",
    "    for word in words:\n",
    "        word = word.lower()  # Lowercase to match the lexicon\n",
    "        if word in nrc_vad_dict:\n",
    "            vad_values = nrc_vad_dict[word]\n",
    "            valence_scores.append(vad_values['Valence'])\n",
    "            arousal_scores.append(vad_values['Arousal'])\n",
    "            dominance_scores.append(vad_values['Dominance'])\n",
    "    if not valence_scores:\n",
    "        return {'Valence': 0, 'Arousal': 0, 'Dominance': 0}\n",
    "\n",
    "    valence_avg = sum(valence_scores) / len(valence_scores)\n",
    "    arousal_avg = sum(arousal_scores) / len(arousal_scores)\n",
    "    dominance_avg = sum(dominance_scores) / len(dominance_scores)\n",
    "    return {'Valence': valence_avg, 'Arousal': arousal_avg, 'Dominance': dominance_avg}\n",
    "\n",
    "df['VAD_Scores'] = df['STATUS'].apply( lambda x: get_vad_scores(x))\n",
    "df[['Valence', 'Arousal', 'Dominance']] = pd.DataFrame(df['VAD_Scores'].tolist(), index=df.index)\n",
    "df.drop(columns=['VAD_Scores'], inplace=True)\n",
    "logging.info(f'NRC-VAD shape={df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_lexicon = pd.read_csv('data/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', names=[\"word\", \"emotion\", \"association\"],sep=\"\\t\", header=None)\n",
    "# Filter out words that have no association with emotions (association == 0)\n",
    "nrc_lexicon = nrc_lexicon[nrc_lexicon['association'] == 1]\n",
    "# nrc_lexicon.drop(columns=['association'], inplace=True)\n",
    "nrc_pivot = nrc_lexicon.pivot(index=\"word\", columns=\"emotion\", values=\"association\").fillna(0).astype(int)\n",
    "# nrc_pivot.head(2)\n",
    "nltk.download('punkt')\n",
    "def get_emotion_counts(text, lexicon):\n",
    "    # print(text)\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    emotion_count = defaultdict(int)\n",
    "    for word in words:\n",
    "        if word in lexicon.index:\n",
    "            for emotion in lexicon.columns:\n",
    "                emotion_count[emotion] += lexicon.loc[word, emotion]\n",
    "    return emotion_count\n",
    "emotion_counts_list = df['STATUS'].apply(lambda x: get_emotion_counts(x, nrc_pivot))\n",
    "emotion_counts_df = pd.DataFrame(emotion_counts_list.tolist())\n",
    "emotion_counts_df.fillna(0, inplace=True)\n",
    "emotion_counts_df = emotion_counts_df.astype(int)\n",
    "df = pd.concat([df, emotion_counts_df], axis=1)\n",
    "logging.info(f'NRC-Emotion shape={df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def find_sentiment(text):\n",
    "    # print(text)\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    sc = vs['compound']\n",
    "    # emo = 'pos' if sc >= 0.05 else 'neu' if -0.05 < sc < 0.05 else 'neg'\n",
    "    return sc\n",
    "df[['sent_score']] = df['STATUS'].apply(lambda x: pd.Series(find_sentiment(x)))\n",
    "logging.info(f'VADER shape={df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TFIDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # Step 1: Initialize the TfidfVectorizer\n",
    "# # You can specify parameters like max_features, ngram_range, etc., based on your needs\n",
    "# tfidf = TfidfVectorizer(max_features=100, stop_words='english')  # Adjust max_features as necessary\n",
    "\n",
    "# # Step 2: Fit and transform the 'STATUS' column\n",
    "# # This step converts the text in 'STATUS' to TF-IDF features\n",
    "# tfidf_matrix = tfidf.fit_transform(df['STATUS'].astype(str))  # Ensure 'STATUS' column is in string format\n",
    "\n",
    "# # Step 3: Convert the TF-IDF matrix into a DataFrame\n",
    "# # The resulting matrix is sparse, so we'll convert it to a DataFrame with feature names\n",
    "# tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# # Step 4: Optionally, merge the TF-IDF features back with your original DataFrame\n",
    "# # This will add the new TF-IDF feature columns to your existing DataFrame\n",
    "# df = pd.concat([df, tfidf_df], axis=1)\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_embeddings(df, model_name, batch_size=8):\n",
    "    logging.info(f'Embeding : {model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_texts = df['STATUS'][i:i + batch_size].tolist()\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings_list.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings_list)\n",
    "    \n",
    "# bert_embeddings = get_embeddings(df, 'bert-base-uncased', batch_size=2)\n",
    "roberta_embeddings = get_embeddings(df, 'roberta-base', batch_size=2)\n",
    "# berttweet_embeddings = get_embeddings(df, 'vinai/bertweet-base', batch_size=2)\n",
    "# xlnet_embeddings = get_embeddings(df, 'xlnet-base-cased', batch_size=2)\n",
    "# df['bert_embeddings'] = list(bert_embeddings)\n",
    "df['roberta_embeddings'] = list(roberta_embeddings)\n",
    "# df['berttweet_embeddings'] = list(berttweet_embeddings)\n",
    "# df['xlnet_embeddings'] = list(xlnet_embeddings)\n",
    "# df[['STATUS', 'bert_embeddings', 'roberta_embeddings', 'berttweet_embeddings', 'xlnet_embeddings']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'data/my_personality_all_embs.csv'\n",
    "# df.to_csv(filename)\n",
    "# logging.info(f'saving to {filename}')\n",
    "# # df = pd.read_csv(\"data/my_personality_all_embs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()\n",
    "# dff.fillna(value=np.nan, inplace=True)\n",
    "# numerical_columns = dff.select_dtypes(include=[np.number]).columns\n",
    "# dff[numerical_columns] = dff[numerical_columns].fillna(dff[numerical_columns].mean())\n",
    "dff.fillna(value=0, inplace=True)\n",
    "logging.info(f'Total shape={dff.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "    def forward(self, x):\n",
    "        query = x[:, -1:, :]  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        scores = torch.bmm(query, x.transpose(1, 2))  # Shape: (batch_size, 1, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.bmm(attention_weights, x)  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, do_attention=True, dropout_rate=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.do_attention = do_attention\n",
    "        self.attention = DotProductAttention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim) \n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "    def forward(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            x = x.unsqueeze(1)  \n",
    "        if self.do_attention:\n",
    "            context_vector, attention_weights = self.attention(x)\n",
    "            context_vector = self.layer_norm1(context_vector)\n",
    "        else:\n",
    "            context_vector = x\n",
    "        lstm_output, _ = self.lstm(context_vector)     \n",
    "        lstm_output = self.layer_norm2(lstm_output)\n",
    "        last_hidden_state = lstm_output[:, -1, :]  # Shape: (batch_size, hidden_dim * 2)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        output = self.fc(last_hidden_state)  \n",
    "        return output\n",
    "\n",
    "# # Test model Initialize model\n",
    "# model = BiLSTMClassifier(input_dim=768, hidden_dim=128, output_dim=5, num_layers=2)\n",
    "# input_data = torch.randn(2, 5, 768)  # Example input (batch_size=32, seq_len=50, input_dim=768)\n",
    "# output= model(input_data)\n",
    "# print(output.shape)  # Expected output: (batch_size, output_dim)\n",
    "\n",
    "def train_val_dl_models(model, train_loader, val_loader, max_grad_norm=1.0, epochs=16, lr=0.001):\n",
    "    logging.info(f'{model.__class__.__name__}')\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for t, labels in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            outputs= model(t)\n",
    "            loss = criterion(outputs, labels)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        model.eval()  \n",
    "        val_preds, val_labels, val_loss = [], [], 0\n",
    "        with torch.no_grad():  \n",
    "            for inputs, labels in val_loader:\n",
    "                outputs= model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()  \n",
    "                val_preds.append(torch.sigmoid(outputs))  \n",
    "                val_labels.append(labels) \n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "        val_preds = (val_preds > 0.5).float() \n",
    "        val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "        if epoch % 4 == 0:\n",
    "            logging.info(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {total_loss / len(train_loader):.4f}, 'f'Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    return val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_training_class:\n",
    "    def __init__(self, dff):\n",
    "        self.dff = dff\n",
    "        # self.output_df = pd.DataFrame()\n",
    "     \n",
    "    def prepare_dataset(self, embedding_type, target_col):\n",
    "        all_cols = self.dff.columns\n",
    "        remove_cols = ['STATUS', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "        emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "        stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "        if embedding_type:\n",
    "            contextual_embeddings = np.array(self.dff[embedding_type].tolist())\n",
    "        scaler = StandardScaler() \n",
    "        stat_features = self.dff[stat_cols]\n",
    "        stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "        X = np.concatenate([stat_features_scaled, contextual_embeddings] if embedding_type else [stat_features_scaled], axis=1)\n",
    "        y = np.array(self.dff[[target_col]]) \n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        print(self.X_train.shape, self.y_train.shape)\n",
    "        self.input_dim = self.X_train.shape[1]\n",
    "        # X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "        # logging.info(f'Train  size: {X_train.shape}, Val size: {X_val.shape}, Test  size: {X_test.shape}')\n",
    "        X_train_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(self.y_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(self.X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(self.y_val, dtype=torch.float32)\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    def init_models(self):\n",
    "        self.svm_model = SVC(kernel='linear')\n",
    "        self.lr_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        self.rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "        self.bilstm_model = BiLSTMClassifier(input_dim=self.input_dim, hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.5)\n",
    "        self.mlp_model = MLP(input_size=self.input_dim, hidden_size=128, output_size=1, dropout_rate=0.3)\n",
    "    \n",
    "    def fit_validate_and_generate_acc_scr(self):\n",
    "        self.svm_model.fit(self.X_train, self.y_train)\n",
    "        self.lr_model.fit(self.X_train, self.y_train)\n",
    "        self.rf_model.fit(self.X_train, self.y_train)\n",
    "        self.xgb_model.fit(self.X_train, self.y_train)\n",
    "        mlp_acc = train_val_dl_models(self.mlp_model, self.train_loader, self.val_loader)\n",
    "        bilstm_acc = train_val_dl_models(self.bilstm_model, self.train_loader, self.val_loader)\n",
    "\n",
    "        svm_y_pred = self.svm_model.predict(self.X_val)\n",
    "        lr_y_pred = self.lr_model.predict(self.X_val)\n",
    "        rf_y_pred = self.rf_model.predict(self.X_val)\n",
    "        xgb_y_pred = self.xgb_model.predict(self.X_val)\n",
    "        svm_accuracy = accuracy_score(self.y_val, svm_y_pred)\n",
    "        lr_accuracy = accuracy_score(self.y_val, lr_y_pred)\n",
    "        rf_accuracy = accuracy_score(self.y_val, rf_y_pred)\n",
    "        xgb_accuracy = accuracy_score(self.y_val, xgb_y_pred)\n",
    "\n",
    "        logging.info(f'SVM Val Acc: {svm_accuracy:.2f}')\n",
    "        logging.info(f'LR Val Acc: {lr_accuracy:.2f}')\n",
    "        logging.info(f'RF Val Acc: {rf_accuracy:.2f}')\n",
    "        logging.info(f'SGBoost Val Acc: {xgb_accuracy:.2f}')\n",
    "        logging.info(f'MLP Val Acc: {mlp_acc:.2f}')\n",
    "        logging.info(f'BiLSTM Val Acc: {bilstm_acc:.2f}')\n",
    "\n",
    "    # def test_model():\n",
    "    #     model.eval()\n",
    "    #     X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    #     with torch.no_grad():  \n",
    "    #         outputs = model(X_test_tensor)\n",
    "    #     outputs = torch.sigmoid(outputs)\n",
    "    #     preds = (outputs > 0.5).float()\n",
    "    #     preds_np = preds.numpy()\n",
    "    #     y_test_np = y_test  # If y_test is already in numpy format, otherwise y_test.numpy()\n",
    "    #     accuracy = accuracy_score(y_test_np, preds_np)\n",
    "    #     precision = precision_score(y_test_np, preds_np)\n",
    "    #     recall = recall_score(y_test_np, preds_np)\n",
    "    #     f1 = f1_score(y_test_np, preds_np)\n",
    "            # Print metrics\n",
    "            # print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    def train_all_models(self, embedding_type):\n",
    "        logging.info(f'Training started with {embedding_model} Embedding')\n",
    "        for target_cols in ['cOPN', 'cCON', 'cEXT', 'cAGR', 'cNEU']:\n",
    "            logging.info(10*'-')\n",
    "            logging.info(f'Trait: {target_cols}')\n",
    "            logging.info(10*'-')        \n",
    "            self.prepare_dataset(embedding_type, target_cols)\n",
    "            self.init_models()\n",
    "            self.fit_validate_and_generate_acc_scr()\n",
    "            logging.info(70*'>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train = My_training_class(dff)\n",
    "my_train.train_all_models(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiLSTMClsAttn(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, do_attention=True, dropout_rate=0.5):\n",
    "#         super(BiLSTMClsAttn, self).__init__()\n",
    "#         self.do_attention = do_attention\n",
    "#         self.attention = DotProductAttention(hidden_dim)\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout_rate)\n",
    "#         self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "#         self.layer_norm1 = nn.LayerNorm(input_dim) \n",
    "#         self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)  \n",
    "#         self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if len(x.size()) == 2:\n",
    "#             x = x.unsqueeze(1)  \n",
    "#         lstm_output, _ = self.lstm(x)     \n",
    "#         lstm_output = self.layer_norm2(lstm_output)\n",
    "#         last_hidden_state = lstm_output[:, -1, :]  # Shape: (batch_size, hidden_dim * 2)\n",
    "#         last_hidden_state = self.dropout(last_hidden_state)\n",
    "#         context_vector, attention_weights = self.attention(last_hidden_state)\n",
    "#         context_vector = self.layer_norm1(context_vector)\n",
    "#         output = self.fc(context_vector)  \n",
    "#         return output\n",
    "\n",
    "# model = BiLSTMClsAttn(input_dim=768, hidden_dim=128, output_dim=1, num_layers=2)\n",
    "# input_data = torch.randn(2, 5, 768)  # Example input (batch_size=32, seq_len=50, input_dim=768)\n",
    "# output= model(input_data)\n",
    "# print(output.shape)  # Expected output: (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BiLSTMClassifier(input_dim=X_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.5)\n",
    "# # model = MLP(input_size=X_train.shape[1], hidden_size=128, output_size=1, dropout_rate=0.3)\n",
    "# criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "# num_epochs = 16\n",
    "# max_grad_norm=1.0\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     for t, labels in train_loader:\n",
    "#         optimizer.zero_grad()  \n",
    "#         outputs= model(t)\n",
    "#         # print(outputs.shape, labels.shape, t.shape)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     # Validation phase\n",
    "#     model.eval()  \n",
    "#     val_preds = []\n",
    "#     val_labels = []\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():  \n",
    "#         for inputs, labels in val_loader:\n",
    "#             outputs= model(inputs)\n",
    "#             val_loss += criterion(outputs, labels).item()  \n",
    "#             val_preds.append(torch.sigmoid(outputs))  \n",
    "#             val_labels.append(labels) \n",
    "#     val_preds = torch.cat(val_preds)\n",
    "#     val_labels = torch.cat(val_labels)\n",
    "#     val_preds = (val_preds > 0.5).float() \n",
    "#     val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "#           f'Train Loss: {total_loss / len(train_loader):.4f}, '\n",
    "#           f'Validation Loss: {val_loss / len(val_loader):.4f}, '\n",
    "#           f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():  \n",
    "    outputs = model(X_test_tensor)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "preds = (outputs > 0.5).float()\n",
    "preds_np = preds.numpy()\n",
    "y_test_np = y_test  # If y_test is already in numpy format, otherwise y_test.numpy()\n",
    "accuracy = accuracy_score(y_test_np, preds_np)\n",
    "precision = precision_score(y_test_np, preds_np)\n",
    "recall = recall_score(y_test_np, preds_np)\n",
    "f1 = f1_score(y_test_np, preds_np)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dff, test_size=0.1, random_state=42)\n",
    "print(f'Train set size: {train_df.shape}')\n",
    "print(f'Test set size: {test_df.shape}')all_cols = dff.columns\n",
    "\n",
    "# label_cols = ['cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "label_cols = [\"cCON\"]\n",
    "remove_cols = ['#AUTHID', 'STATUS', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN', 'DATE']\n",
    "emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "\n",
    "stat_features = train_df[stat_cols]\n",
    "bert_embeddings = np.array(train_df[\"bert_embeddings\"].tolist())\n",
    "roberta_embeddings = np.array(train_df[\"roberta_embeddings\"].tolist())\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "X1 = np.concatenate([stat_features_scaled, bert_embeddings], axis=1)\n",
    "X2 = np.concatenate([stat_features_scaled, roberta_embeddings], axis=1)\n",
    "y = np.array(train_df[label_cols]) \n",
    "\n",
    "# Split data into train+val and test sets (80% train+val, 20% test)\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1, y, test_size=0.1, random_state=42)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset1 = TensorDataset(torch.tensor(X1_train, dtype=torch.float32), torch.tensor(y1_train, dtype=torch.float32))\n",
    "val_dataset1 = TensorDataset(torch.tensor(X1_val, dtype=torch.float32), torch.tensor(y1_val, dtype=torch.float32))\n",
    "train_dataset2 = TensorDataset(torch.tensor(X2_train, dtype=torch.float32), torch.tensor(y2_train, dtype=torch.float32))\n",
    "val_dataset2 = TensorDataset(torch.tensor(X2_val, dtype=torch.float32), torch.tensor(y2_val, dtype=torch.float32))\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=32, shuffle=True)\n",
    "val_loader1 = DataLoader(val_dataset1, batch_size=32, shuffle=False)\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=32, shuffle=True)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Aveaging\n",
    "model1 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "model2 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "\n",
    "# model1 = MLP(input_size=X1_train.shape[1], hidden_size=128, output_size=1)\n",
    "# model2 = MLP(input_size=X2_train.shape[1], hidden_size=128, output_size=1)\n",
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "num_epochs = 12\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for (t1, labels1), (t2, labels2) in zip(train_loader1, train_loader2):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        outputs1 = model1(t1)\n",
    "        outputs2 = model2(t2)\n",
    "        \n",
    "        avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "        loss = criterion(avg_outputs, labels1)  \n",
    "        torch.nn.utils.clip_grad_norm_(model1.parameters(), max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        total_loss += loss.item()\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss1 = 0\n",
    "    val_loss2 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (inputs1, labels1), (inputs2, labels2) in zip(val_loader1, val_loader2):\n",
    "            outputs1 = model1(inputs1)\n",
    "            outputs2 = model2(inputs2)\n",
    "            avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "            val_loss1 += criterion(outputs1, labels1).item()\n",
    "            val_loss2 += criterion(outputs2, labels2).item()\n",
    "            val_preds.append(torch.sigmoid(avg_outputs))\n",
    "            val_labels.append(labels1)\n",
    "    \n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_preds = (val_preds > 0.5).float()\n",
    "    val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss Model1: {total_loss / len(train_loader1):.4f}, '\n",
    "          f'Validation Loss Model1: {val_loss1 / len(val_loader1):.4f}, '\n",
    "          f'Validation Loss Model2: {val_loss2 / len(val_loader2):.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
