{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging, sys\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "logging.basicConfig(filename=f'log/jp_log_{timestamp}.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df = pd.read_csv('data/pandora_to_big5.csv')\n",
    "# main_df.drop(columns=['#AUTHID'], inplace=True)\n",
    "# cols = ['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']\n",
    "# for col in cols:\n",
    "#     mean_value = main_df[col].mean()\n",
    "#     main_df[f'{col}'] = main_df[col] > mean_value\n",
    "#     main_df[cols] = main_df[cols].replace({True: 1, False: 0})  \n",
    "# liwc_df = pd.read_csv('data/LIWC_pandora_to_big5_oct_24.csv')\n",
    "# liwc_df = liwc_df.drop(['Unnamed: 0', '#AUTHID', 'STATUS', 'cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON', ], axis=1)\n",
    "# df = pd.concat([main_df, liwc_df], axis=1)\n",
    "# logging.info(f'Merged main and liwc files, Shape:{df.shape}')\n",
    "\n",
    "# cols = ['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']\n",
    "# main_df = pd.read_csv('data/mypersonality.csv', encoding='Windows-1252')\n",
    "# main_df.drop(columns=['#AUTHID', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'DATE', 'NETWORKSIZE', 'BETWEENNESS', 'NBETWEENNESS', 'DENSITY', 'BROKERAGE', 'NBROKERAGE','TRANSITIVITY'], inplace=True)\n",
    "# main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']] = main_df[['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']].replace({'y': 1, 'n': 0})\n",
    "# liwc_df = pd.read_csv('data/LIWC_mypersonality_oct_2.csv')\n",
    "# liwc_df = liwc_df.drop(['Unnamed: 0', '#AUTHID', 'ColumnID', 'STATUS'], axis=1)\n",
    "# df = pd.concat([main_df, liwc_df], axis=1)\n",
    "# logging.info(f'Merged main and liwc files, Shape:{df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmaharja/anaconda3/envs/gpu2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44976, 139)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PersonaClassifier import My_training_class\n",
    "my_train = My_training_class('data/pandora_processed_train.csv', model_list=['mlp'], embedding_model='roberta-base')\n",
    "# my_train.process_raw_files('data/pandora_to_big5.csv', 'data/LIWC_pandora_to_big5_oct_24.csv' , False)\n",
    "my_train.preprocess_data()\n",
    "my_train.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "def train(model, train_loader, val_loader, max_grad_norm=1.0, epochs=20, lr=0.001):\n",
    "    logging.info(f'{model.__class__.__name__}; lr={lr}')\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for t, labels in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            outputs= model(t)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        model.eval()  \n",
    "        # print(outputs.shape, labels.shape, outputs.squeeze().shape)\n",
    "        val_preds, val_labels, val_scores, val_loss = [], [], [], 0\n",
    "        with torch.no_grad():  \n",
    "            for inputs, labels in val_loader:\n",
    "                outputs= model(inputs)\n",
    "                # print(outputs.shape, labels.shape, outputs.squeeze().shape)\n",
    "                val_loss += criterion(outputs.squeeze(), labels).item()  \n",
    "                val_preds.append(torch.sigmoid(outputs))  \n",
    "                val_labels.append(labels) \n",
    "                val_scores.append(outputs)\n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "        val_preds = (val_preds > 0.5).float() \n",
    "        val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "        if epoch % 4 == 0:\n",
    "            logging.info(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {total_loss / len(train_loader):.4f}, 'f'Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    return val_accuracy, val_preds, torch.cat(val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44976, 139)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif, mutual_info_classif, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "class FeatureSelection:\n",
    "    def variance_selection(X, threshold=0.16): #(.8 * (1 - .8))\n",
    "        logging.info(f'Variance Feature Selection. Threshold used: {threshold}')\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        selected_features = selector.fit_transform(X)\n",
    "        return  X.columns[selector.get_support()]\n",
    "\n",
    "    def mutual_info_selection(X, y, threshold=0.001):\n",
    "        logging.info(f'Mutual Info Feature Selection. Threshold used: {threshold}')\n",
    "        mutual_info = mutual_info_classif(X, y)\n",
    "        mutual_info = pd.Series(mutual_info)\n",
    "        mutual_info.index = X.columns\n",
    "        ig_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Information Gain': mutual_info\n",
    "        }).sort_values(by='Information Gain', ascending=False)\n",
    "        return ig_df[ig_df['Information Gain'] > threshold]['Feature'].values\n",
    "\n",
    "    def filter_selection(X, y):\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "        selector.fit(X, y)\n",
    "        return X.columns[selector.get_support()]\n",
    "\n",
    "    def hybrid_selection(X, y):\n",
    "        logging.info(f'Hybrid method combining SelectFromModel and Logistic Regression')\n",
    "        selector = SelectFromModel(LogisticRegression(penalty=\"l2\", C=0.1))\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        return X.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_cols = list (set(my_train.df.columns) - set(['STATUS', 'original', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']) - set(['roberta_embeddings']))\n",
    "default_selection = FeatureSelection.variance_selection(my_train.df[stat_cols])\n",
    "len(default_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_selection = FeatureSelection.mutual_info_selection(my_train.df[stat_cols], my_train.df['cCON'], 0.001)\n",
    "len(mutual_info_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_selection = FeatureSelection.hybrid_selection(my_train.df[stat_cols], my_train.df['cCON'])\n",
    "len(hybrid_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def prepare_dataset(my_train, target_col, test_size, batch_size, selected_features):\n",
    "    scaler = StandardScaler() \n",
    "    stat_features_scaled = scaler.fit_transform(my_train.df[selected_features])\n",
    "    X = np.concatenate([stat_features_scaled, my_train.contextual_embeddings] if my_train.contextual_embeddings is not None else [stat_features_scaled], axis=1)\n",
    "    y = np.array(my_train.df[[target_col]]) \n",
    "    y = y.ravel() \n",
    "    logging.info(y.shape)\n",
    "    my_train.X_train, my_train.X_val, my_train.y_train, my_train.y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    X_train_tensor = torch.tensor(my_train.X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(my_train.y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(my_train.X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(my_train.y_val, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    my_train.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    my_train.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # logging.info(my_train.X_train.shape, my_train.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PersonaClassifier import MLP, BiLSTMClassifier, train_val_dl_models\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset(my_train, 'cCON', test_size=0.1, batch_size=16, selected_features=default_selection)\n",
    "my_train.mlp_model = MLP(input_size=my_train.X_train.shape[1], hidden_size=128, output_size=1, dropout_rate=0.3)\n",
    "mlp_acc, y_pred, y_scores = train(my_train.mlp_model, my_train.train_loader, my_train.val_loader, max_grad_norm=1.0, epochs=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset(my_train, 'cCON', test_size=0.1, batch_size=16, selected_features=mutual_info_selection)\n",
    "my_train.mlp_model = MLP(input_size=my_train.X_train.shape[1], hidden_size=128, output_size=1, dropout_rate=0.3)\n",
    "mlp_acc, y_pred, y_scores = train(my_train.mlp_model, my_train.train_loader, my_train.val_loader, max_grad_norm=1.0, epochs=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset(my_train, 'cCON', test_size=0.1, batch_size=16, selected_features=hybrid_selection)\n",
    "my_train.mlp_model = MLP(input_size=my_train.X_train.shape[1], hidden_size=128, output_size=1, dropout_rate=0.5)\n",
    "mlp_acc, y_pred, y_scores = train(my_train.mlp_model, my_train.train_loader, my_train.val_loader, max_grad_norm=1.0, epochs=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmaharja/anaconda3/envs/gpu2/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "combined_features = FeatureSelection.hybrid_selection(my_train.df[mutual_info_selection], my_train.df['cCON'])\n",
    "prepare_dataset(my_train, 'cCON', test_size=0.1, batch_size=16, selected_features=combined_features)\n",
    "my_train.mlp_model = MLP(input_size=my_train.X_train.shape[1], hidden_size=128, output_size=1, dropout_rate=0.5)\n",
    "mlp_acc, y_pred, y_scores = train(my_train.mlp_model, my_train.train_loader, my_train.val_loader, max_grad_norm=1.0, epochs=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "    def forward(self, x):\n",
    "        query = x[:, -1:, :]  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        scores = torch.bmm(query, x.transpose(1, 2))  # Shape: (batch_size, 1, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.bmm(attention_weights, x)  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, do_attention=True, dropout_rate=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.do_attention = do_attention\n",
    "        self.attention = DotProductAttention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim) \n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "    def forward(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            x = x.unsqueeze(1)  \n",
    "        if self.do_attention:\n",
    "            context_vector, attention_weights = self.attention(x)\n",
    "            context_vector = self.layer_norm1(context_vector)\n",
    "        else:\n",
    "            context_vector = x\n",
    "        lstm_output, _ = self.lstm(context_vector)     \n",
    "        lstm_output = self.layer_norm2(lstm_output)\n",
    "        last_hidden_state = lstm_output[:, -1, :]  # Shape: (batch_size, hidden_dim * 2)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        output = self.fc(last_hidden_state)  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = \"checkpoint/MLP_2024-11-22_13-17-32.pth\"\n",
    "model = MLP(input_size=903, hidden_size=128, output_size=1)\n",
    "model.load_state_dict(torch.load(model_filepath))\n",
    "model.eval()\n",
    "print(f\"Model loaded from {model_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():  \n",
    "    outputs = model(X_test_tensor)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "preds = (outputs > 0.5).float()\n",
    "preds_np = preds.numpy()\n",
    "y_test_np = y_test  # If y_test is already in numpy format, otherwise y_test.numpy()\n",
    "accuracy = accuracy_score(y_test_np, preds_np)\n",
    "precision = precision_score(y_test_np, preds_np)\n",
    "recall = recall_score(y_test_np, preds_np)\n",
    "f1 = f1_score(y_test_np, preds_np)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dff, test_size=0.1, random_state=42)\n",
    "print(f'Train set size: {train_df.shape}')\n",
    "print(f'Test set size: {test_df.shape}')all_cols = dff.columns\n",
    "\n",
    "# label_cols = ['cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "label_cols = [\"cCON\"]\n",
    "remove_cols = ['#AUTHID', 'STATUS', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN', 'DATE']\n",
    "emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "\n",
    "stat_features = train_df[stat_cols]\n",
    "bert_embeddings = np.array(train_df[\"bert_embeddings\"].tolist())\n",
    "roberta_embeddings = np.array(train_df[\"roberta_embeddings\"].tolist())\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "X1 = np.concatenate([stat_features_scaled, bert_embeddings], axis=1)\n",
    "X2 = np.concatenate([stat_features_scaled, roberta_embeddings], axis=1)\n",
    "y = np.array(train_df[label_cols]) \n",
    "\n",
    "# Split data into train+val and test sets (80% train+val, 20% test)\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1, y, test_size=0.1, random_state=42)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset1 = TensorDataset(torch.tensor(X1_train, dtype=torch.float32), torch.tensor(y1_train, dtype=torch.float32))\n",
    "val_dataset1 = TensorDataset(torch.tensor(X1_val, dtype=torch.float32), torch.tensor(y1_val, dtype=torch.float32))\n",
    "train_dataset2 = TensorDataset(torch.tensor(X2_train, dtype=torch.float32), torch.tensor(y2_train, dtype=torch.float32))\n",
    "val_dataset2 = TensorDataset(torch.tensor(X2_val, dtype=torch.float32), torch.tensor(y2_val, dtype=torch.float32))\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=32, shuffle=True)\n",
    "val_loader1 = DataLoader(val_dataset1, batch_size=32, shuffle=False)\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=32, shuffle=True)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Aveaging\n",
    "model1 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "model2 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "\n",
    "# model1 = MLP(input_size=X1_train.shape[1], hidden_size=128, output_size=1)\n",
    "# model2 = MLP(input_size=X2_train.shape[1], hidden_size=128, output_size=1)\n",
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "num_epochs = 12\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for (t1, labels1), (t2, labels2) in zip(train_loader1, train_loader2):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        outputs1 = model1(t1)\n",
    "        outputs2 = model2(t2)\n",
    "        \n",
    "        avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "        loss = criterion(avg_outputs, labels1)  \n",
    "        torch.nn.utils.clip_grad_norm_(model1.parameters(), max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        total_loss += loss.item()\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss1 = 0\n",
    "    val_loss2 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (inputs1, labels1), (inputs2, labels2) in zip(val_loader1, val_loader2):\n",
    "            outputs1 = model1(inputs1)\n",
    "            outputs2 = model2(inputs2)\n",
    "            avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "            val_loss1 += criterion(outputs1, labels1).item()\n",
    "            val_loss2 += criterion(outputs2, labels2).item()\n",
    "            val_preds.append(torch.sigmoid(avg_outputs))\n",
    "            val_labels.append(labels1)\n",
    "    \n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_preds = (val_preds > 0.5).float()\n",
    "    val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss Model1: {total_loss / len(train_loader1):.4f}, '\n",
    "          f'Validation Loss Model1: {val_loss1 / len(val_loader1):.4f}, '\n",
    "          f'Validation Loss Model2: {val_loss2 / len(val_loader2):.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
