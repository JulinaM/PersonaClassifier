{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re,os, glob, traceback, nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_personality_file = 'data/mypersonality.csv'\n",
    "main_df = pd.read_csv(my_personality_file, encoding='Windows-1252')\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>Segment</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>BigWords</th>\n",
       "      <th>Dic</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  Segment  WC  Analytic  Clout  Authentic  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4        1   5      99.0    NaN        NaN   \n",
       "\n",
       "   Tone  WPS  BigWords   Dic  ...  nonflu  filler  AllPunc  Period  Comma  \\\n",
       "0  99.0  5.0      20.0  80.0  ...     0.0     0.0     20.0    20.0    0.0   \n",
       "\n",
       "   QMark  Exclam  Apostro  OtherP  Emoji  \n",
       "0    0.0     0.0      0.0     0.0    0.0  \n",
       "\n",
       "[1 rows x 120 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_file = 'data/LIWC_mypersonality_oct_2.csv'\n",
    "liwc_df = pd.read_csv(liwc_file)\n",
    "liwc_df = liwc_df.drop(['Unnamed: 0', 'ColumnID', 'Text'], axis=1)\n",
    "# liwc_df.fillna(value=np.nan, inplace=True)\n",
    "# numerical_columns = liwc_df.select_dtypes(include=[np.number]).columns\n",
    "# liwc_df[numerical_columns] = liwc_df[numerical_columns].fillna(liwc_df[numerical_columns].mean())\n",
    "# liwc_df[numerical_columns] = liwc_df[numerical_columns].fillna(0)\n",
    "# liwc_df.fillna(value=0, inplace=True)\n",
    "liwc_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 137)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.merge(main_df, liwc_df, on=\"#AUTHID\", how=\"left\")\n",
    "df = pd.concat([main_df, liwc_df], axis=1)\n",
    "df = df.drop(['#AUTHID', 'DATE'], axis=1)\n",
    "str(list(df.columns))\n",
    "# df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU cAGR  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4    n    y    n   \n",
       "\n",
       "  cCON  ... Period  Comma  QMark  Exclam  Apostro  OtherP  Emoji  Valence  \\\n",
       "0    n  ...   20.0    0.0    0.0     0.0      0.0     0.0    0.0    0.765   \n",
       "\n",
       "   Arousal  Dominance  \n",
       "0    0.623      0.398  \n",
       "\n",
       "[1 rows x 140 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrc_vad = pd.read_csv('data/NRC-VAD-Lexicon/NRC-VAD-Lexicon.csv', sep=\"\\t\")  \n",
    "nrc_vad_dict = nrc_vad.set_index('Word').to_dict(orient='index')\n",
    "def get_vad_scores(text):\n",
    "    words = text.split()\n",
    "    valence_scores, arousal_scores, dominance_scores = [], [], []\n",
    "    for word in words:\n",
    "        word = word.lower()  # Lowercase to match the lexicon\n",
    "        if word in nrc_vad_dict:\n",
    "            vad_values = nrc_vad_dict[word]\n",
    "            valence_scores.append(vad_values['Valence'])\n",
    "            arousal_scores.append(vad_values['Arousal'])\n",
    "            dominance_scores.append(vad_values['Dominance'])\n",
    "    if not valence_scores:\n",
    "        return {'Valence': 0, 'Arousal': 0, 'Dominance': 0}\n",
    "\n",
    "    valence_avg = sum(valence_scores) / len(valence_scores)\n",
    "    arousal_avg = sum(arousal_scores) / len(arousal_scores)\n",
    "    dominance_avg = sum(dominance_scores) / len(dominance_scores)\n",
    "    return {'Valence': valence_avg, 'Arousal': arousal_avg, 'Dominance': dominance_avg}\n",
    "\n",
    "df['VAD_Scores'] = df['STATUS'].apply( lambda x: get_vad_scores(x))\n",
    "df[['Valence', 'Arousal', 'Dominance']] = pd.DataFrame(df['VAD_Scores'].tolist(), index=df.index)\n",
    "df.drop(columns=['VAD_Scores'], inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jmaharja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU cAGR  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4    n    y    n   \n",
       "\n",
       "  cCON  ... anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
       "0    n  ...     0             0        0     0    0         0         0   \n",
       "\n",
       "   sadness  surprise  trust  \n",
       "0        0         0      0  \n",
       "\n",
       "[1 rows x 150 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrc_lexicon = pd.read_csv('data/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', names=[\"word\", \"emotion\", \"association\"],sep=\"\\t\", header=None)\n",
    "# Filter out words that have no association with emotions (association == 0)\n",
    "nrc_lexicon = nrc_lexicon[nrc_lexicon['association'] == 1]\n",
    "# nrc_lexicon.drop(columns=['association'], inplace=True)\n",
    "nrc_pivot = nrc_lexicon.pivot(index=\"word\", columns=\"emotion\", values=\"association\").fillna(0).astype(int)\n",
    "# nrc_pivot.head(2)\n",
    "nltk.download('punkt')\n",
    "def get_emotion_counts(text, lexicon):\n",
    "    # print(text)\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    emotion_count = defaultdict(int)\n",
    "    for word in words:\n",
    "        if word in lexicon.index:\n",
    "            for emotion in lexicon.columns:\n",
    "                emotion_count[emotion] += lexicon.loc[word, emotion]\n",
    "    return emotion_count\n",
    "emotion_counts_list = df['STATUS'].apply(lambda x: get_emotion_counts(x, nrc_pivot))\n",
    "emotion_counts_df = pd.DataFrame(emotion_counts_list.tolist())\n",
    "emotion_counts_df.fillna(0, inplace=True)\n",
    "emotion_counts_df = emotion_counts_df.astype(int)\n",
    "df = pd.concat([df, emotion_counts_df], axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU cAGR  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4    n    y    n   \n",
       "\n",
       "  cCON  ... anticipation  disgust  fear  joy  negative  positive  sadness  \\\n",
       "0    n  ...            0        0     0    0         0         0        0   \n",
       "\n",
       "   surprise  trust  sent_score  \n",
       "0         0      0      0.4215  \n",
       "\n",
       "[1 rows x 151 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def find_sentiment(text):\n",
    "    # print(text)\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    sc = vs['compound']\n",
    "    # emo = 'pos' if sc >= 0.05 else 'neu' if -0.05 < sc < 0.05 else 'neg'\n",
    "    return sc\n",
    "df[['sent_score']] = df['STATUS'].apply(lambda x: pd.Series(find_sentiment(x)))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>watching</th>\n",
       "      <th>way</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>wishes</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>yay</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU cAGR  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4    n    y    n   \n",
       "\n",
       "  cCON  ... watching  way  week  weekend  wishes  work  working  world  yay  \\\n",
       "0    n  ...      0.0  0.0   0.0      0.0     0.0   0.0      0.0    0.0  0.0   \n",
       "\n",
       "   year  \n",
       "0   0.0  \n",
       "\n",
       "[1 rows x 251 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Step 1: Initialize the TfidfVectorizer\n",
    "# You can specify parameters like max_features, ngram_range, etc., based on your needs\n",
    "tfidf = TfidfVectorizer(max_features=100, stop_words='english')  # Adjust max_features as necessary\n",
    "\n",
    "# Step 2: Fit and transform the 'STATUS' column\n",
    "# This step converts the text in 'STATUS' to TF-IDF features\n",
    "tfidf_matrix = tfidf.fit_transform(df['STATUS'].astype(str))  # Ensure 'STATUS' column is in string format\n",
    "\n",
    "# Step 3: Convert the TF-IDF matrix into a DataFrame\n",
    "# The resulting matrix is sparse, so we'll convert it to a DataFrame with feature names\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Step 4: Optionally, merge the TF-IDF features back with your original DataFrame\n",
    "# This will add the new TF-IDF feature columns to your existing DataFrame\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmaharja/anaconda3/envs/gpu2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_embeddings(df, model_name, batch_size=8):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_texts = df['STATUS'][i:i + batch_size].tolist()\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings_list.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings_list)\n",
    "    \n",
    "bert_embeddings = get_embeddings(df, 'bert-base-uncased', batch_size=2)\n",
    "roberta_embeddings = get_embeddings(df, 'roberta-base', batch_size=2)\n",
    "berttweet_embeddings = get_embeddings(df, 'vinai/bertweet-base', batch_size=2)\n",
    "xlnet_embeddings = get_embeddings(df, 'xlnet-base-cased', batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bert_embeddings'] = list(bert_embeddings)\n",
    "df['roberta_embeddings'] = list(roberta_embeddings)\n",
    "df['berttweet_embeddings'] = list(berttweet_embeddings)\n",
    "df['xlnet_embeddings'] = list(xlnet_embeddings)\n",
    "# df[['STATUS', 'bert_embeddings', 'roberta_embeddings', 'berttweet_embeddings', 'xlnet_embeddings']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9917, 255)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>wishes</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>yay</th>\n",
       "      <th>year</th>\n",
       "      <th>bert_embeddings</th>\n",
       "      <th>roberta_embeddings</th>\n",
       "      <th>berttweet_embeddings</th>\n",
       "      <th>xlnet_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.5658656, -0.034683075, -0.3991114, -0.2992...</td>\n",
       "      <td>[-0.11116652, 0.09479402, -0.031377178, -0.120...</td>\n",
       "      <td>[-0.21966171, 0.3103059, 0.10933973, -0.048810...</td>\n",
       "      <td>[-0.5085498, 1.0680441, -0.029119104, -0.40934...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN  cEXT  cNEU  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4     0     1   \n",
       "\n",
       "   cAGR  cCON  ...  wishes  work  working  world  yay  year  \\\n",
       "0     0     0  ...     0.0   0.0      0.0    0.0  0.0   0.0   \n",
       "\n",
       "                                     bert_embeddings  \\\n",
       "0  [-0.5658656, -0.034683075, -0.3991114, -0.2992...   \n",
       "\n",
       "                                  roberta_embeddings  \\\n",
       "0  [-0.11116652, 0.09479402, -0.031377178, -0.120...   \n",
       "\n",
       "                                berttweet_embeddings  \\\n",
       "0  [-0.21966171, 0.3103059, 0.10933973, -0.048810...   \n",
       "\n",
       "                                    xlnet_embeddings  \n",
       "0  [-0.5085498, 1.0680441, -0.029119104, -0.40934...  \n",
       "\n",
       "[1 rows x 255 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = df.copy()\n",
    "columns_to_change = ['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']\n",
    "dff[columns_to_change] = dff[columns_to_change].replace({'y': 1, 'n': 0})\n",
    "# dff.fillna(value=np.nan, inplace=True)\n",
    "# numerical_columns = dff.select_dtypes(include=[np.number]).columns\n",
    "# dff[numerical_columns] = dff[numerical_columns].fillna(dff[numerical_columns].mean())\n",
    "dff.fillna(value=0, inplace=True)\n",
    "print(dff.shape)\n",
    "dff.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['swear', 'needs', 'lol', 'having', 'reward', 'good', 'got', 'adverb', 'power', 'god', 'people', 'propname', 'Analytic', 'politic', 'space', 'does', 'allure', 'night', 'wishes', 'yay', 'life', 'emo_anger', 'house', 'motion', 'maybe', 'comm', 'ppron', 'affiliation', 'mental', 'hours', 'trust', 'long', 'auxverb', 'acquire', 'make', 'Lifestyle', 'fulfill', 'certitude', 'gonna', 'differ', 'looking', 'class', 'verb', 'shehe', 'relig', 'let', 'Social', 'finally', 'family', 'they', 'money', 'facebook', 'better', 'work', 'i', 'number', 'emo_anx', 'Physical', 'emo_pos', 'know', 'wants', 'function', 'Comma', 'risk', 'Dominance', 'hope', 'thinks', 'loves', 'article', 'netspeak', 'tone_neg', 'sent_score', 'going', 'Period', 'WC', 'QMark', 'Emoji', 'substances', 'quantity', 'allnone', 'Culture', 'emo_sad', 'Perception', 'positive', 'wait', 'tone_pos', 'Cognition', 'friday', 'male', 'ipron', 'health', 'BigWords', 'thank', 'you', 'friends', 'getting', 'car', 'little', 'female', 'anger', 'AllPunc', 'oh', 'Affect', 'OtherP', 'did', 'polite', 'death', 'doesn', 'best', 'fun', 'nonflu', 'new', 'lack', 'Arousal', 'det', 'today', 'memory', 'tech', 'trying', 'morning', 'just', 'like', 'Tone', 'filler', 'bad', 'prep', 'thing', 'sick', 'BROKERAGE', 'weekend', 'negative', 'TRANSITIVITY', 'food', 'home', 'ready', 'im', 'cause', 'things', 'visual', 'awesome', 'Exclam', 'man', 'Dic', 'sadness', 'thanks', 'tonight', 'need', 've', 'NBROKERAGE', 'surprise', 'leisure', 'old', 'Clout', 'emotion', 'anticipation', 'curiosity', 'love', 'happy', 'negate', 'auditory', 'world', 'tentat', 'year', 'time', 'working', 'start', 'BETWEENNESS', 'bed', 'school', 'attention', 'joy', 'sexual', 'snow', 'sleep', 'feeling', 'focuspresent', 'cogproc', 'Drives', 'come', 'feel', 'friend', 'making', 'want', 'DENSITY', 'disgust', 'way', 'NBETWEENNESS', 'assent', 'NETWORKSIZE', 'focuspast', 'think', 'Segment', 'tired', 'great', 'prosocial', 'excited', 'conj', 'say', 'll', 'moral', 'days', 'insight', 'tomorrow', 'don', 'fatigue', 'Linguistic', 'christmas', 'day', 'pronoun', 'achieve', 'birthday', 'Apostro', 'watch', 'adj', 'conflict', 'wellness', 'Conversation', 'Valence', 'we', 'socrefs', 'discrep', 'really', 'illness', 'socbehav', 'focusfuture', 'emo_neg', 'right', 'week', 'fear', 'watching', 'WPS', 'la', 'doing', 'Authentic', 'ethnicity']\",\n",
       " 235)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols = dff.columns\n",
    "# liwc_df_cols = list(set(liwc_df.columns) - set(['#AUTHID', 'Text']))\n",
    "label_cols = ['cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "remove_cols = ['#AUTHID', 'STATUS', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN', 'DATE']\n",
    "emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "str(stat_cols), len(stat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (7933, 1008)\n",
      "Validation set size: (992, 1008)\n",
      "Test set size: (992, 1008)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "stat_features = dff[stat_cols]\n",
    "contextual_embeddings = np.array(dff[\"bert_embeddings\"].tolist())\n",
    "\n",
    "scaler = StandardScaler()  # This normalizes each feature to have mean=0 and std=1\n",
    "stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "X = np.concatenate([stat_features_scaled, contextual_embeddings], axis=1)\n",
    "y = np.array(dff[label_cols]) \n",
    "\n",
    "# Split data into train+val and test sets (80% train+val, 20% test)\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42)\n",
    "print(f'Train set size: {X_train.shape}')\n",
    "print(f'Validation set size: {X_val.shape}')\n",
    "print(f'Test set size: {X_test.shape}')\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print(torch.isnan(X_train_tensor).any())  # Check for NaNs in training input\n",
    "print(torch.isinf(X_train_tensor).any())   # Check for Infs in training input\n",
    "print(torch.isnan(X_val_tensor).any())     # Check for NaNs in validation input\n",
    "print(torch.isinf(X_val_tensor).any())      # Check for Infs in validation input\n",
    "# shape : 240 + 768 = 1008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>swear</th>\n",
       "      <th>needs</th>\n",
       "      <th>lol</th>\n",
       "      <th>having</th>\n",
       "      <th>reward</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>adverb</th>\n",
       "      <th>power</th>\n",
       "      <th>god</th>\n",
       "      <th>...</th>\n",
       "      <th>emo_neg</th>\n",
       "      <th>right</th>\n",
       "      <th>week</th>\n",
       "      <th>fear</th>\n",
       "      <th>watching</th>\n",
       "      <th>WPS</th>\n",
       "      <th>la</th>\n",
       "      <th>doing</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.68</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9917 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      swear  needs  lol  having  reward  good  got  adverb  power  god  ...  \\\n",
       "0       0.0    0.0  0.0     0.0     0.0   0.0  0.0    0.00    0.0  0.0  ...   \n",
       "1       0.0    0.0  0.0     0.0     0.0   0.0  0.0   15.38    0.0  0.0  ...   \n",
       "2       0.0    0.0  0.0     0.0     0.0   0.0  0.0    0.00    0.0  0.0  ...   \n",
       "3       0.0    0.0  0.0     0.0     0.0   0.0  0.0   11.11    0.0  0.0  ...   \n",
       "4       0.0    0.0  0.0     0.0     0.0   0.0  0.0    0.00    0.0  0.0  ...   \n",
       "...     ...    ...  ...     ...     ...   ...  ...     ...    ...  ...  ...   \n",
       "9912    0.0    0.0  0.0     0.0     0.0   0.0  0.0    0.00    0.0  0.0  ...   \n",
       "9913    0.0    0.0  0.0     0.0     0.0   0.0  0.0    0.00    0.0  0.0  ...   \n",
       "9914    0.0    0.0  0.0     0.0     0.0   0.0  0.0    0.00    0.0  0.0  ...   \n",
       "9915    0.0    0.0  0.0     0.0     0.0   0.0  0.0    0.00    0.0  0.0  ...   \n",
       "9916    0.0    0.0  0.0     0.0     0.0   0.0  0.0    6.06    0.0  0.0  ...   \n",
       "\n",
       "      emo_neg  right  week  fear  watching    WPS   la  doing  Authentic  \\\n",
       "0        0.00    0.0   0.0     0       0.0   5.00  0.0    0.0       0.00   \n",
       "1        0.00    0.0   0.0     0       0.0  13.00  0.0    0.0       1.00   \n",
       "2        7.69    0.0   0.0     1       0.0   8.67  0.0    0.0       2.93   \n",
       "3        0.00    0.0   0.0     0       0.0   9.00  0.0    0.0      99.00   \n",
       "4        0.00    0.0   0.0     0       0.0   1.50  0.0    0.0      99.00   \n",
       "...       ...    ...   ...   ...       ...    ...  ...    ...        ...   \n",
       "9912     0.00    0.0   0.0     0       0.0   5.00  0.0    0.0      63.35   \n",
       "9913     0.00    0.0   0.0     0       0.0   5.00  0.0    0.0       1.00   \n",
       "9914     0.00    0.0   0.0     0       0.0   8.00  0.0    0.0       0.00   \n",
       "9915     0.00    0.0   0.0     0       0.0   4.00  0.0    0.0       0.00   \n",
       "9916     0.00    0.0   0.0     0       0.0  16.50  0.0    0.0      86.68   \n",
       "\n",
       "      ethnicity  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "9912        0.0  \n",
       "9913        0.0  \n",
       "9914        0.0  \n",
       "9915        0.0  \n",
       "9916        0.0  \n",
       "\n",
       "[9917 rows x 240 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)  # Dropout to prevent overfitting\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.6635, Validation Loss: 0.6556, Validation Accuracy: 0.0726\n",
      "Epoch [2/20], Train Loss: 0.6431, Validation Loss: 0.6436, Validation Accuracy: 0.1099\n",
      "Epoch [3/20], Train Loss: 0.6220, Validation Loss: 0.6319, Validation Accuracy: 0.1079\n",
      "Epoch [4/20], Train Loss: 0.6024, Validation Loss: 0.6350, Validation Accuracy: 0.1472\n",
      "Epoch [5/20], Train Loss: 0.5743, Validation Loss: 0.6521, Validation Accuracy: 0.1411\n",
      "Epoch [6/20], Train Loss: 0.5463, Validation Loss: 0.6468, Validation Accuracy: 0.1472\n",
      "Epoch [7/20], Train Loss: 0.5180, Validation Loss: 0.6645, Validation Accuracy: 0.1623\n",
      "Epoch [8/20], Train Loss: 0.4873, Validation Loss: 0.6942, Validation Accuracy: 0.1734\n",
      "Epoch [9/20], Train Loss: 0.4522, Validation Loss: 0.7129, Validation Accuracy: 0.1754\n",
      "Epoch [10/20], Train Loss: 0.4294, Validation Loss: 0.7388, Validation Accuracy: 0.1653\n",
      "Epoch [11/20], Train Loss: 0.3981, Validation Loss: 0.7639, Validation Accuracy: 0.1744\n",
      "Epoch [12/20], Train Loss: 0.3800, Validation Loss: 0.8240, Validation Accuracy: 0.1714\n",
      "Epoch [13/20], Train Loss: 0.3654, Validation Loss: 0.8167, Validation Accuracy: 0.1593\n",
      "Epoch [14/20], Train Loss: 0.3397, Validation Loss: 0.8444, Validation Accuracy: 0.1573\n",
      "Epoch [15/20], Train Loss: 0.3246, Validation Loss: 0.8691, Validation Accuracy: 0.1623\n",
      "Epoch [16/20], Train Loss: 0.3193, Validation Loss: 0.9122, Validation Accuracy: 0.1552\n",
      "Epoch [17/20], Train Loss: 0.2988, Validation Loss: 0.9201, Validation Accuracy: 0.1552\n",
      "Epoch [18/20], Train Loss: 0.2856, Validation Loss: 0.9333, Validation Accuracy: 0.1583\n",
      "Epoch [19/20], Train Loss: 0.2813, Validation Loss: 0.9533, Validation Accuracy: 0.1643\n",
      "Epoch [20/20], Train Loss: 0.2704, Validation Loss: 0.9817, Validation Accuracy: 0.1532\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = MLP(input_size=X_train.shape[1], hidden_size=128, output_size=5)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "max_grad_norm=1.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)\n",
    "        if torch.isnan(outputs).any():\n",
    "            print(\"NaN detected in model outputs!\")\n",
    "            break\n",
    "        loss = criterion(outputs, labels)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(loss.item)\n",
    "        total_loss += loss.item()\n",
    "    # Validation phase\n",
    "    model.eval()  \n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()  # Calculate validation loss\n",
    "            val_preds.append(torch.sigmoid(outputs))  # Collect predictions\n",
    "            val_labels.append(labels)  # Collect true labels\n",
    "    # Concatenate predictions and labels for accuracy calculation\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_preds = (val_preds > 0.5).float()  # Threshold to get binary predictions\n",
    "    val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "    # Print epoch results\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {total_loss / len(train_loader):.4f}, '\n",
    "          f'Validation Loss: {val_loss / len(val_loader):.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2440\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():  \n",
    "    outputs = model(X_test_tensor)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "preds = (outputs > 0.5).float()\n",
    "preds_np = preds.numpy()\n",
    "y_test_np = y_test  # If y_test is already in numpy format, otherwise y_test.numpy()\n",
    "accuracy = accuracy_score(y_test_np, preds_np)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim * 2, hidden_dim * 2)  # For bidirectional\n",
    "        self.Ua = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.Va = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch_size, seq_len, hidden_dim * 2)\n",
    "        score = self.Va(torch.tanh(self.Wa(lstm_output)))  # Shape: (batch_size, seq_len, 1)\n",
    "        attention_weights = torch.softmax(score, dim=1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.bmm(attention_weights.permute(0, 2, 1), lstm_output)  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if len(x.size()) == 2:\n",
    "        #     x = x.unsqueeze(1)  # Add batch dimension to make it (1, seq_len, input_dim)\n",
    "        # h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_dim).to(x.device)\n",
    "        # c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_dim).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x)     \n",
    "        context_vector, attention_weights = self.attention(lstm_out)\n",
    "        context_vector = context_vector.squeeze(1)  # Shape: (batch_size, hidden_dim * 2)\n",
    "        # out = lstm_out[:, -1, :]# Take the last time-step output\n",
    "        out = self.fc(context_vector)\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = BiLSTMClassifier(input_dim=768, hidden_dim=128, output_dim=5, num_layers=2)\n",
    "input_data = torch.randn(32, 50, 768)  # Example input (batch_size=32, seq_len=50, input_dim=768)\n",
    "\n",
    "# Forward pass\n",
    "output, _ = model(input_data)\n",
    "print(output.shape)  # Expected output: (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = BiLSTMClassifier(input_dim=X_train.shape[1], hidden_dim=128, output_dim=5,num_layers=2)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "max_grad_norm=1.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for t, labels in train_loader:\n",
    "        optimizer.zero_grad()  \n",
    "        outputs, _ = model(t)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels.shape)\n",
    "        # print(t.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # Validation phase\n",
    "    model.eval()  \n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()  # Calculate validation loss\n",
    "            val_preds.append(torch.sigmoid(outputs))  # Collect predictions\n",
    "            val_labels.append(labels)  # Collect true labels\n",
    "    # Concatenate predictions and labels for accuracy calculation\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_preds = (val_preds > 0.5).float()  # Threshold to get binary predictions\n",
    "    val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "    # Print epoch results\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {total_loss / len(train_loader):.4f}, '\n",
    "          f'Validation Loss: {val_loss / len(val_loader):.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
