{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re,os, glob, traceback, nltk\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_personality_file = 'data/mypersonality.csv'\n",
    "main_df = pd.read_csv(my_personality_file, encoding='Windows-1252')\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>Segment</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>BigWords</th>\n",
       "      <th>Dic</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  Segment  WC  Analytic  Clout  Authentic  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4        1   5      99.0    NaN        NaN   \n",
       "\n",
       "   Tone  WPS  BigWords   Dic  ...  nonflu  filler  AllPunc  Period  Comma  \\\n",
       "0  99.0  5.0      20.0  80.0  ...     0.0     0.0     20.0    20.0    0.0   \n",
       "\n",
       "   QMark  Exclam  Apostro  OtherP  Emoji  \n",
       "0    0.0     0.0      0.0     0.0    0.0  \n",
       "\n",
       "[1 rows x 120 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_file = 'data/LIWC_mypersonality_oct_2.csv'\n",
    "liwc_df = pd.read_csv(liwc_file)\n",
    "liwc_df = liwc_df.drop(['Unnamed: 0', 'ColumnID', 'Text'], axis=1)\n",
    "# liwc_df.fillna(value=np.nan, inplace=True)\n",
    "# numerical_columns = liwc_df.select_dtypes(include=[np.number]).columns\n",
    "# liwc_df[numerical_columns] = liwc_df[numerical_columns].fillna(liwc_df[numerical_columns].mean())\n",
    "# liwc_df[numerical_columns] = liwc_df[numerical_columns].fillna(0)\n",
    "# liwc_df.fillna(value=0, inplace=True)\n",
    "liwc_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 137)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.merge(main_df, liwc_df, on=\"#AUTHID\", how=\"left\")\n",
    "df = pd.concat([main_df, liwc_df], axis=1)\n",
    "df = df.drop(['#AUTHID', 'DATE'], axis=1)\n",
    "str(list(df.columns))\n",
    "# df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU cAGR  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4    n    y    n   \n",
       "\n",
       "  cCON  ... Period  Comma  QMark  Exclam  Apostro  OtherP  Emoji  Valence  \\\n",
       "0    n  ...   20.0    0.0    0.0     0.0      0.0     0.0    0.0    0.765   \n",
       "\n",
       "   Arousal  Dominance  \n",
       "0    0.623      0.398  \n",
       "\n",
       "[1 rows x 140 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrc_vad = pd.read_csv('data/NRC-VAD-Lexicon/NRC-VAD-Lexicon.csv', sep=\"\\t\")  \n",
    "nrc_vad_dict = nrc_vad.set_index('Word').to_dict(orient='index')\n",
    "def get_vad_scores(text):\n",
    "    words = text.split()\n",
    "    valence_scores, arousal_scores, dominance_scores = [], [], []\n",
    "    for word in words:\n",
    "        word = word.lower()  # Lowercase to match the lexicon\n",
    "        if word in nrc_vad_dict:\n",
    "            vad_values = nrc_vad_dict[word]\n",
    "            valence_scores.append(vad_values['Valence'])\n",
    "            arousal_scores.append(vad_values['Arousal'])\n",
    "            dominance_scores.append(vad_values['Dominance'])\n",
    "    if not valence_scores:\n",
    "        return {'Valence': 0, 'Arousal': 0, 'Dominance': 0}\n",
    "\n",
    "    valence_avg = sum(valence_scores) / len(valence_scores)\n",
    "    arousal_avg = sum(arousal_scores) / len(arousal_scores)\n",
    "    dominance_avg = sum(dominance_scores) / len(dominance_scores)\n",
    "    return {'Valence': valence_avg, 'Arousal': arousal_avg, 'Dominance': dominance_avg}\n",
    "\n",
    "df['VAD_Scores'] = df['STATUS'].apply( lambda x: get_vad_scores(x))\n",
    "df[['Valence', 'Arousal', 'Dominance']] = pd.DataFrame(df['VAD_Scores'].tolist(), index=df.index)\n",
    "df.drop(columns=['VAD_Scores'], inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jmaharja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU cAGR  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4    n    y    n   \n",
       "\n",
       "  cCON  ... anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
       "0    n  ...     0             0        0     0    0         0         0   \n",
       "\n",
       "   sadness  surprise  trust  \n",
       "0        0         0      0  \n",
       "\n",
       "[1 rows x 150 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrc_lexicon = pd.read_csv('data/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', names=[\"word\", \"emotion\", \"association\"],sep=\"\\t\", header=None)\n",
    "# Filter out words that have no association with emotions (association == 0)\n",
    "nrc_lexicon = nrc_lexicon[nrc_lexicon['association'] == 1]\n",
    "# nrc_lexicon.drop(columns=['association'], inplace=True)\n",
    "nrc_pivot = nrc_lexicon.pivot(index=\"word\", columns=\"emotion\", values=\"association\").fillna(0).astype(int)\n",
    "# nrc_pivot.head(2)\n",
    "nltk.download('punkt')\n",
    "def get_emotion_counts(text, lexicon):\n",
    "    # print(text)\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    emotion_count = defaultdict(int)\n",
    "    for word in words:\n",
    "        if word in lexicon.index:\n",
    "            for emotion in lexicon.columns:\n",
    "                emotion_count[emotion] += lexicon.loc[word, emotion]\n",
    "    return emotion_count\n",
    "emotion_counts_list = df['STATUS'].apply(lambda x: get_emotion_counts(x, nrc_pivot))\n",
    "emotion_counts_df = pd.DataFrame(emotion_counts_list.tolist())\n",
    "emotion_counts_df.fillna(0, inplace=True)\n",
    "emotion_counts_df = emotion_counts_df.astype(int)\n",
    "df = pd.concat([df, emotion_counts_df], axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU cAGR  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4    n    y    n   \n",
       "\n",
       "  cCON  ... anticipation  disgust  fear  joy  negative  positive  sadness  \\\n",
       "0    n  ...            0        0     0    0         0         0        0   \n",
       "\n",
       "   surprise  trust  sent_score  \n",
       "0         0      0      0.4215  \n",
       "\n",
       "[1 rows x 151 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def find_sentiment(text):\n",
    "    # print(text)\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    sc = vs['compound']\n",
    "    # emo = 'pos' if sc >= 0.05 else 'neu' if -0.05 < sc < 0.05 else 'neg'\n",
    "    return sc\n",
    "df[['sent_score']] = df['STATUS'].apply(lambda x: pd.Series(find_sentiment(x)))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Step 1: Initialize the TfidfVectorizer\n",
    "# You can specify parameters like max_features, ngram_range, etc., based on your needs\n",
    "tfidf = TfidfVectorizer(max_features=100, stop_words='english')  # Adjust max_features as necessary\n",
    "\n",
    "# Step 2: Fit and transform the 'STATUS' column\n",
    "# This step converts the text in 'STATUS' to TF-IDF features\n",
    "tfidf_matrix = tfidf.fit_transform(df['STATUS'].astype(str))  # Ensure 'STATUS' column is in string format\n",
    "\n",
    "# Step 3: Convert the TF-IDF matrix into a DataFrame\n",
    "# The resulting matrix is sparse, so we'll convert it to a DataFrame with feature names\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Step 4: Optionally, merge the TF-IDF features back with your original DataFrame\n",
    "# This will add the new TF-IDF feature columns to your existing DataFrame\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_embeddings(df, model_name, batch_size=8):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_texts = df['STATUS'][i:i + batch_size].tolist()\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings_list.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings_list)\n",
    "    \n",
    "# bert_embeddings = get_embeddings(df, 'bert-base-uncased', batch_size=2)\n",
    "# roberta_embeddings = get_embeddings(df, 'roberta-base', batch_size=2)\n",
    "berttweet_embeddings = get_embeddings(df, 'vinai/bertweet-base', batch_size=2)\n",
    "xlnet_embeddings = get_embeddings(df, 'xlnet-base-cased', batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['bert_embeddings'] = list(bert_embeddings)\n",
    "# df['roberta_embeddings'] = list(roberta_embeddings)\n",
    "df['berttweet_embeddings'] = list(berttweet_embeddings)\n",
    "df['xlnet_embeddings'] = list(xlnet_embeddings)\n",
    "# df[['STATUS', 'bert_embeddings', 'roberta_embeddings', 'berttweet_embeddings', 'xlnet_embeddings']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/my_personality_all_embs.csv\")\n",
    "# df = pd.read_csv(\"data/my_personality_all_embs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9917, 155)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>...</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>sent_score</th>\n",
       "      <th>bert_embeddings</th>\n",
       "      <th>roberta_embeddings</th>\n",
       "      <th>berttweet_embeddings</th>\n",
       "      <th>xlnet_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>[-0.5658656, -0.034683075, -0.3991114, -0.2992...</td>\n",
       "      <td>[-0.11116652, 0.09479402, -0.031377178, -0.120...</td>\n",
       "      <td>[-0.21966171, 0.3103059, 0.10933973, -0.048810...</td>\n",
       "      <td>[-0.5085498, 1.0680441, -0.029119104, -0.40934...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  sEXT  sNEU  sAGR  sCON  sOPN  cEXT  cNEU  \\\n",
       "0  likes the sound of thunder.  2.65   3.0  3.15  3.25   4.4     0     1   \n",
       "\n",
       "   cAGR  cCON  ...  negative  positive  sadness  surprise  trust  sent_score  \\\n",
       "0     0     0  ...         0         0        0         0      0      0.4215   \n",
       "\n",
       "                                     bert_embeddings  \\\n",
       "0  [-0.5658656, -0.034683075, -0.3991114, -0.2992...   \n",
       "\n",
       "                                  roberta_embeddings  \\\n",
       "0  [-0.11116652, 0.09479402, -0.031377178, -0.120...   \n",
       "\n",
       "                                berttweet_embeddings  \\\n",
       "0  [-0.21966171, 0.3103059, 0.10933973, -0.048810...   \n",
       "\n",
       "                                    xlnet_embeddings  \n",
       "0  [-0.5085498, 1.0680441, -0.029119104, -0.40934...  \n",
       "\n",
       "[1 rows x 155 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = df.copy()\n",
    "columns_to_change = ['cOPN', 'cEXT', 'cNEU', 'cAGR', 'cCON']\n",
    "dff[columns_to_change] = dff[columns_to_change].replace({'y': 1, 'n': 0})\n",
    "# dff.fillna(value=np.nan, inplace=True)\n",
    "# numerical_columns = dff.select_dtypes(include=[np.number]).columns\n",
    "# dff[numerical_columns] = dff[numerical_columns].fillna(dff[numerical_columns].mean())\n",
    "dff.fillna(value=0, inplace=True)\n",
    "print(dff.shape)\n",
    "dff.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['health', 'want', 'Apostro', 'home', 'emo_sad', 'conj', 'fulfill', 'relig', 'cause', 'leisure', 'space', 'male', 'emo_pos', 'mental', 'BROKERAGE', 'Lifestyle', 'Linguistic', 'female', 'anger', 'polite', 'netspeak', 'illness', 'Emoji', 'emotion', 'Physical', 'Exclam', 'Valence', 'substances', 'Dominance', 'differ', 'verb', 'ethnicity', 'wellness', 'i', 'positive', 'prosocial', 'auxverb', 'shehe', 'filler', 'Affect', 'curiosity', 'negative', 'allure', 'trust', 'adj', 'power', 'affiliation', 'Drives', 'TRANSITIVITY', 'need', 'nonflu', 'food', 'money', 'Tone', 'surprise', 'they', 'auditory', 'swear', 'motion', 'focusfuture', 'disgust', 'WC', 'quantity', 'death', 'Period', 'OtherP', 'DENSITY', 'function', 'sent_score', 'certitude', 'discrep', 'Authentic', 'AllPunc', 'Comma', 'assent', 'socbehav', 'ipron', 'tone_neg', 'prep', 'tentat', 'risk', 'comm', 'Conversation', 'focuspast', 'achieve', 'Cognition', 'you', 'number', 'Social', 'anticipation', 'cogproc', 'attention', 'sexual', 'family', 'emo_neg', 'Segment', 'tech', 'lack', 'Arousal', 'politic', 'socrefs', 'joy', 'feeling', 'det', 'WPS', 'tone_pos', 'conflict', 'BigWords', 'Analytic', 'insight', 'work', 'Clout', 'fear', 'QMark', 'emo_anger', 'time', 'NETWORKSIZE', 'NBETWEENNESS', 'emo_anx', 'negate', 'acquire', 'moral', 'sadness', 'memory', 'ppron', 'adverb', 'NBROKERAGE', 'Dic', 'allnone', 'Culture', 'reward', 'we', 'Perception', 'friend', 'visual', 'fatigue', 'BETWEENNESS', 'article', 'focuspresent', 'pronoun']\",\n",
       " 140)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols = dff.columns\n",
    "# liwc_df_cols = list(set(liwc_df.columns) - set(['#AUTHID', 'Text']))\n",
    "# label_cols = ['cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "label_cols = [\"cNEU\"]\n",
    "\n",
    "remove_cols = ['#AUTHID', 'STATUS', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN', 'DATE']\n",
    "emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "str(stat_cols), len(stat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (7933, 908)\n",
      "Validation set size: (992, 908)\n",
      "Test set size: (992, 908)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "stat_features = dff[stat_cols]\n",
    "contextual_embeddings = np.array(dff[\"roberta_embeddings\"].tolist())\n",
    "\n",
    "scaler = StandardScaler()  # This normalizes each feature to have mean=0 and std=1\n",
    "stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "X = np.concatenate([stat_features_scaled, contextual_embeddings], axis=1)\n",
    "y = np.array(dff[label_cols]) \n",
    "\n",
    "# Split data into train+val and test sets (80% train+val, 20% test)\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42)\n",
    "print(f'Train set size: {X_train.shape}')\n",
    "print(f'Validation set size: {X_val.shape}')\n",
    "print(f'Test set size: {X_test.shape}')\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print(torch.isnan(X_train_tensor).any())  # Check for NaNs in training input\n",
    "print(torch.isinf(X_train_tensor).any())   # Check for Infs in training input\n",
    "print(torch.isnan(X_val_tensor).any())     # Check for NaNs in validation input\n",
    "print(torch.isinf(X_val_tensor).any())      # Check for Infs in validation input\n",
    "# shape : 240 + 768 = 1008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "    def forward(self, x):\n",
    "        query = x[:, -1:, :]  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        scores = torch.bmm(query, x.transpose(1, 2))  # Shape: (batch_size, 1, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.bmm(attention_weights, x)  # Shape: (batch_size, 1, hidden_dim * 2)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, do_attention=True, dropout_rate=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.do_attention = do_attention\n",
    "        self.attention = DotProductAttention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim) \n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "    def forward(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            x = x.unsqueeze(1)  \n",
    "        if self.do_attention:\n",
    "            context_vector, attention_weights = self.attention(x)\n",
    "            context_vector = self.layer_norm1(context_vector)\n",
    "        else:\n",
    "            context_vector = x\n",
    "        lstm_output, _ = self.lstm(context_vector)     \n",
    "        lstm_output = self.layer_norm2(lstm_output)\n",
    "        last_hidden_state = lstm_output[:, -1, :]  # Shape: (batch_size, hidden_dim * 2)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        output = self.fc(last_hidden_state)  \n",
    "        return output\n",
    "\n",
    "# Test model Initialize model\n",
    "model = BiLSTMClassifier(input_dim=768, hidden_dim=128, output_dim=5, num_layers=2)\n",
    "input_data = torch.randn(2, 5, 768)  # Example input (batch_size=32, seq_len=50, input_dim=768)\n",
    "output= model(input_data)\n",
    "print(output.shape)  # Expected output: (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[290], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m BiLSTMClsAttn(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     25\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m768\u001b[39m)  \u001b[38;5;66;03m# Example input (batch_size=32, seq_len=50, input_dim=768)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m output\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[290], line 19\u001b[0m, in \u001b[0;36mBiLSTMClsAttn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m lstm_output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Shape: (batch_size, hidden_dim * 2)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(last_hidden_state)\n\u001b[0;32m---> 19\u001b[0m context_vector, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m context_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(context_vector)\n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(context_vector)  \n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[250], line 5\u001b[0m, in \u001b[0;36mDotProductAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 5\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size, 1, hidden_dim * 2)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(query, x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# Shape: (batch_size, 1, seq_len)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, seq_len, 1)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "class BiLSTMClsAttn(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional=True, do_attention=True, dropout_rate=0.5):\n",
    "        super(BiLSTMClsAttn, self).__init__()\n",
    "        self.do_attention = do_attention\n",
    "        self.attention = DotProductAttention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim) \n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            x = x.unsqueeze(1)  \n",
    "        lstm_output, _ = self.lstm(x)     \n",
    "        lstm_output = self.layer_norm2(lstm_output)\n",
    "        last_hidden_state = lstm_output[:, -1, :]  # Shape: (batch_size, hidden_dim * 2)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        context_vector, attention_weights = self.attention(last_hidden_state)\n",
    "        context_vector = self.layer_norm1(context_vector)\n",
    "        output = self.fc(context_vector)  \n",
    "        return output\n",
    "\n",
    "model = BiLSTMClsAttn(input_dim=768, hidden_dim=128, output_dim=1, num_layers=2)\n",
    "input_data = torch.randn(2, 5, 768)  # Example input (batch_size=32, seq_len=50, input_dim=768)\n",
    "output= model(input_data)\n",
    "print(output.shape)  # Expected output: (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/16], Train Loss: 0.6586, Validation Loss: 0.6277, Validation Accuracy: 0.6724\n",
      "Epoch [2/16], Train Loss: 0.6097, Validation Loss: 0.5917, Validation Accuracy: 0.6966\n",
      "Epoch [3/16], Train Loss: 0.5808, Validation Loss: 0.5877, Validation Accuracy: 0.7107\n",
      "Epoch [4/16], Train Loss: 0.5508, Validation Loss: 0.5775, Validation Accuracy: 0.7046\n",
      "Epoch [5/16], Train Loss: 0.5174, Validation Loss: 0.5598, Validation Accuracy: 0.7198\n",
      "Epoch [6/16], Train Loss: 0.4826, Validation Loss: 0.5845, Validation Accuracy: 0.7097\n",
      "Epoch [7/16], Train Loss: 0.4501, Validation Loss: 0.5764, Validation Accuracy: 0.7238\n",
      "Epoch [8/16], Train Loss: 0.4088, Validation Loss: 0.5818, Validation Accuracy: 0.7369\n",
      "Epoch [9/16], Train Loss: 0.3829, Validation Loss: 0.6393, Validation Accuracy: 0.7339\n",
      "Epoch [10/16], Train Loss: 0.3552, Validation Loss: 0.6011, Validation Accuracy: 0.7369\n",
      "Epoch [11/16], Train Loss: 0.3239, Validation Loss: 0.6636, Validation Accuracy: 0.7319\n",
      "Epoch [12/16], Train Loss: 0.3091, Validation Loss: 0.6651, Validation Accuracy: 0.7319\n",
      "Epoch [13/16], Train Loss: 0.2790, Validation Loss: 0.7184, Validation Accuracy: 0.7429\n",
      "Epoch [14/16], Train Loss: 0.2634, Validation Loss: 0.6911, Validation Accuracy: 0.7288\n",
      "Epoch [15/16], Train Loss: 0.2397, Validation Loss: 0.7264, Validation Accuracy: 0.7268\n",
      "Epoch [16/16], Train Loss: 0.2198, Validation Loss: 0.7644, Validation Accuracy: 0.7379\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = BiLSTMClassifier(input_dim=X_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.5)\n",
    "# model = MLP(input_size=X_train.shape[1], hidden_size=128, output_size=1, dropout_rate=0.3)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "num_epochs = 16\n",
    "max_grad_norm=1.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for t, labels in train_loader:\n",
    "        optimizer.zero_grad()  \n",
    "        outputs= model(t)\n",
    "        # print(outputs.shape, labels.shape, t.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # Validation phase\n",
    "    model.eval()  \n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in val_loader:\n",
    "            outputs= model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()  \n",
    "            val_preds.append(torch.sigmoid(outputs))  \n",
    "            val_labels.append(labels) \n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_preds = (val_preds > 0.5).float() \n",
    "    val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {total_loss / len(train_loader):.4f}, '\n",
    "          f'Validation Loss: {val_loss / len(val_loader):.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7248, Precision: 0.6162, Recall: 0.5351, F1 Score: 0.5728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():  \n",
    "    outputs = model(X_test_tensor)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "preds = (outputs > 0.5).float()\n",
    "preds_np = preds.numpy()\n",
    "y_test_np = y_test  # If y_test is already in numpy format, otherwise y_test.numpy()\n",
    "accuracy = accuracy_score(y_test_np, preds_np)\n",
    "precision = precision_score(y_test_np, preds_np)\n",
    "recall = recall_score(y_test_np, preds_np)\n",
    "f1 = f1_score(y_test_np, preds_np)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['new', 'Authentic', 'right', 'joy', 'det', 'excited', 'illness', 'adj', 'car', 'auditory', 'feel', 'hours', 'old', 'watching', 'Emoji', 'Period', 'they', 'want', 'auxverb', 'socbehav', 'power', 'emo_neg', 'sadness', 'socrefs', 'tomorrow', 'negative', 'family', 'tech', 'Conversation', 'don', 'Social', 'substances', 'filler', 'day', 'emotion', 'night', 'going', 'trust', 'Drives', 'days', 'oh', 'Cognition', 'think', 'need', 'mental', 'ipron', 'memory', 'finally', 'trying', 'Tone', 'gonna', 'WC', 'NETWORKSIZE', 'Physical', 'surprise', 'Arousal', 'god', 'propname', 'sleep', 'focuspresent', 'ppron', 'prep', 'death', 'week', 'yay', 'year', 'Linguistic', 'Apostro', 'did', 'comm', 'politic', 'doing', 'getting', 'friends', 'moral', 'needs', 'come', 'loves', 'fun', 'sexual', 'thank', 'BETWEENNESS', 'better', 'prosocial', 'working', 'sent_score', 'BigWords', 'female', 'bed', 'weekend', 'life', 'motion', 'happy', 'article', 'certitude', 'ready', 'sick', 'best', 'food', 'christmas', 'love', 'nonflu', 'today', 'differ', 've', 'disgust', 'insight', 'lack', 'home', 'birthday', 'conj', 'fear', 'BROKERAGE', 'shehe', 'feeling', 'hope', 'just', 'thinks', 'like', 'cause', 'tone_neg', 'bad', 'looking', 'you', 'male', 'does', 'visual', 'let', 'fatigue', 'NBETWEENNESS', 'swear', 'awesome', 'achieve', 'snow', 'Affect', 'verb', 'man', 'Lifestyle', 'having', 'wellness', 'Exclam', 'affiliation', 'Comma', 'class', 'ethnicity', 'anticipation', 'space', 'lol', 'cogproc', 'Perception', 'risk', 'tone_pos', 'Analytic', 'time', 'Dominance', 'positive', 'know', 'll', 'make', 'tonight', 'emo_anx', 'allure', 'AllPunc', 'watch', 'fulfill', 'emo_sad', 'school', 'quantity', 'friend', 'number', 'QMark', 'morning', 'say', 'wishes', 'conflict', 'people', 'allnone', 'im', 'work', 'reward', 'thanks', 'acquire', 'got', 'things', 'way', 'world', 'money', 'WPS', 'netspeak', 'really', 'OtherP', 'friday', 'long', 'wants', 'thing', 'anger', 'great', 'pronoun', 'emo_anger', 'Valence', 'DENSITY', 'negate', 'making', 'tired', 'relig', 'focusfuture', 'i', 'leisure', 'little', 'Dic', 'house', 'good', 'adverb', 'function', 'tentat', 'start', 'TRANSITIVITY', 'attention', 'Segment', 'we', 'health', 'assent', 'curiosity', 'doesn', 'Culture', 'facebook', 'Clout', 'discrep', 'polite', 'focuspast', 'maybe', 'wait', 'emo_pos', 'NBROKERAGE', 'la']\",\n",
       " 235)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(dff, test_size=0.1, random_state=42)\n",
    "print(f'Train set size: {train_df.shape}')\n",
    "print(f'Test set size: {test_df.shape}')all_cols = dff.columns\n",
    "\n",
    "# label_cols = ['cEXT','cNEU', 'cAGR', 'cCON', 'cOPN']\n",
    "label_cols = [\"cCON\"]\n",
    "remove_cols = ['#AUTHID', 'STATUS', 'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'cEXT','cNEU', 'cAGR', 'cCON', 'cOPN', 'DATE']\n",
    "emb_cols = ['bert_embeddings', 'berttweet_embeddings', 'xlnet_embeddings', 'roberta_embeddings']\n",
    "stat_cols = list (set(all_cols) - set(remove_cols) - set(emb_cols))\n",
    "\n",
    "stat_features = train_df[stat_cols]\n",
    "bert_embeddings = np.array(train_df[\"bert_embeddings\"].tolist())\n",
    "roberta_embeddings = np.array(train_df[\"roberta_embeddings\"].tolist())\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "stat_features_scaled = scaler.fit_transform(stat_features)\n",
    "X1 = np.concatenate([stat_features_scaled, bert_embeddings], axis=1)\n",
    "X2 = np.concatenate([stat_features_scaled, roberta_embeddings], axis=1)\n",
    "y = np.array(train_df[label_cols]) \n",
    "\n",
    "# Split data into train+val and test sets (80% train+val, 20% test)\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1, y, test_size=0.1, random_state=42)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset1 = TensorDataset(torch.tensor(X1_train, dtype=torch.float32), torch.tensor(y1_train, dtype=torch.float32))\n",
    "val_dataset1 = TensorDataset(torch.tensor(X1_val, dtype=torch.float32), torch.tensor(y1_val, dtype=torch.float32))\n",
    "train_dataset2 = TensorDataset(torch.tensor(X2_train, dtype=torch.float32), torch.tensor(y2_train, dtype=torch.float32))\n",
    "val_dataset2 = TensorDataset(torch.tensor(X2_val, dtype=torch.float32), torch.tensor(y2_val, dtype=torch.float32))\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=32, shuffle=True)\n",
    "val_loader1 = DataLoader(val_dataset1, batch_size=32, shuffle=False)\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=32, shuffle=True)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Train Loss Model1: 0.6946, Validation Loss Model1: 0.7014, Validation Loss Model2: 0.7130, Validation Accuracy: 0.5566\n",
      "Epoch [2/12], Train Loss Model1: 0.6414, Validation Loss Model1: 0.7564, Validation Loss Model2: 0.7003, Validation Accuracy: 0.5924\n",
      "Epoch [3/12], Train Loss Model1: 0.5967, Validation Loss Model1: 0.8415, Validation Loss Model2: 0.6924, Validation Accuracy: 0.5969\n",
      "Epoch [4/12], Train Loss Model1: 0.5288, Validation Loss Model1: 0.9173, Validation Loss Model2: 0.6923, Validation Accuracy: 0.6036\n",
      "Epoch [5/12], Train Loss Model1: 0.4329, Validation Loss Model1: 1.1190, Validation Loss Model2: 0.6883, Validation Accuracy: 0.6047\n",
      "Epoch [6/12], Train Loss Model1: 0.3463, Validation Loss Model1: 1.3680, Validation Loss Model2: 0.6925, Validation Accuracy: 0.6036\n",
      "Epoch [7/12], Train Loss Model1: 0.2609, Validation Loss Model1: 1.5066, Validation Loss Model2: 0.6901, Validation Accuracy: 0.6260\n",
      "Epoch [8/12], Train Loss Model1: 0.2033, Validation Loss Model1: 1.9807, Validation Loss Model2: 0.6896, Validation Accuracy: 0.6237\n",
      "Epoch [9/12], Train Loss Model1: 0.1579, Validation Loss Model1: 2.2688, Validation Loss Model2: 0.6886, Validation Accuracy: 0.6159\n",
      "Epoch [10/12], Train Loss Model1: 0.1339, Validation Loss Model1: 2.3395, Validation Loss Model2: 0.6916, Validation Accuracy: 0.6170\n",
      "Epoch [11/12], Train Loss Model1: 0.1023, Validation Loss Model1: 2.4648, Validation Loss Model2: 0.7034, Validation Accuracy: 0.6327\n",
      "Epoch [12/12], Train Loss Model1: 0.0922, Validation Loss Model1: 2.7707, Validation Loss Model2: 0.6888, Validation Accuracy: 0.6215\n"
     ]
    }
   ],
   "source": [
    "#Model Aveaging\n",
    "model1 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "model2 = BiLSTMClassifier(input_dim=X2_train.shape[1], hidden_dim=128, output_dim=1, num_layers=2, bidirectional=True, do_attention=True, dropout_rate=0.3)\n",
    "\n",
    "# model1 = MLP(input_size=X1_train.shape[1], hidden_size=128, output_size=1)\n",
    "# model2 = MLP(input_size=X2_train.shape[1], hidden_size=128, output_size=1)\n",
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "num_epochs = 12\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for (t1, labels1), (t2, labels2) in zip(train_loader1, train_loader2):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        outputs1 = model1(t1)\n",
    "        outputs2 = model2(t2)\n",
    "        \n",
    "        avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "        loss = criterion(avg_outputs, labels1)  \n",
    "        torch.nn.utils.clip_grad_norm_(model1.parameters(), max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        total_loss += loss.item()\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss1 = 0\n",
    "    val_loss2 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (inputs1, labels1), (inputs2, labels2) in zip(val_loader1, val_loader2):\n",
    "            outputs1 = model1(inputs1)\n",
    "            outputs2 = model2(inputs2)\n",
    "            avg_outputs = (outputs1 + outputs2) / 2.0\n",
    "            val_loss1 += criterion(outputs1, labels1).item()\n",
    "            val_loss2 += criterion(outputs2, labels2).item()\n",
    "            val_preds.append(torch.sigmoid(avg_outputs))\n",
    "            val_labels.append(labels1)\n",
    "    \n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_preds = (val_preds > 0.5).float()\n",
    "    val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss Model1: {total_loss / len(train_loader1):.4f}, '\n",
    "          f'Validation Loss Model1: {val_loss1 / len(val_loader1):.4f}, '\n",
    "          f'Validation Loss Model2: {val_loss2 / len(val_loader2):.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
